{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Katy-Kittivibul/Sheffield-spatial-pattern-mining/blob/main/Sheffield_DT_SL_all_distances.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqLx6DiVnTwM"
      },
      "source": [
        "# **Spatial pattern mining of air quality diffusion tubes in Sheffield**\n",
        "\n",
        "This notebook is a part of MSc Data Science dissertation by Kulisara Kittivibul.\n",
        "\n",
        "This study aims to identify the spatial relationship between air quality monitoring diffusion tubes and sensitive locations (e.g. schools, kindergartens, hospitals, etc.) in Sheffield.\n",
        "\n",
        "Dataset: https://sheffield-city-council-open-data-sheffieldcc.hub.arcgis.com/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgucH4kMWdqa"
      },
      "source": [
        "## Import libraries and packages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# update the package for current session\n",
        "!pip install --upgrade jupyter_client"
      ],
      "metadata": {
        "id": "QBykMv5PyAdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvQx_8AQFboK"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install geopandas shapely mlxtend osmnx esda\n",
        "\n",
        "import os\n",
        "import osmnx as ox\n",
        "import geopandas as gpd\n",
        "import folium\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import itertools\n",
        "import branca.colormap as cm\n",
        "import libpysal as lp\n",
        "import folium\n",
        "from collections import defaultdict\n",
        "from folium.plugins import MarkerCluster\n",
        "from esda.moran import Moran, Moran_Local\n",
        "from scipy.spatial import cKDTree\n",
        "from shapely.geometry import Point, MultiPoint, LineString\n",
        "from shapely.ops import nearest_points\n",
        "from mlxtend.frequent_patterns import apriori, association_rules"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import warnings\n",
        "#warnings.filterwarnings(\n",
        "#    \"ignore\",\n",
        "#    message=\"datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version.*\",\n",
        "#    category=DeprecationWarning,\n",
        "#    module='jupyter_client')"
      ],
      "metadata": {
        "id": "MVWoYBfnxLmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqryKBE8UPjP"
      },
      "source": [
        "## Sensitive locations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GRObDtMFrCL"
      },
      "outputs": [],
      "source": [
        "# Define the place of interest\n",
        "place_name = \"Sheffield, UK\"\n",
        "\n",
        "# Define the tags for the sensitive locations you want to retrieve\n",
        "# Refer to the OpenStreetMap Wiki for comprehensive tag documentation:\n",
        "# https://wiki.openstreetmap.org/wiki/Map_features\n",
        "tags = {\"amenity\": [\"hospital\",\n",
        "                    \"university\",\n",
        "                    \"school\",\n",
        "                    \"kindergarten\",\n",
        "                    \"nursing_home\",\n",
        "                    \"childcare\"],\n",
        "        \"healthcare\": \"hospital\"\n",
        "        }\n",
        "\n",
        "print(f\"Retrieving data for {place_name}...\")\n",
        "\n",
        "# 1. Retrieve the data from OpenStreetMap using osmnx\n",
        "sensitive_locations_gdf = ox.features.features_from_place(place_name, tags)\n",
        "\n",
        "print(f\"Retrieved {len(sensitive_locations_gdf)} features.\")\n",
        "print(f\"Initial CRS: {sensitive_locations_gdf.crs}\")\n",
        "\n",
        "# Reset the index to make 'id' accessible columns\n",
        "sensitive_locations_gdf = sensitive_locations_gdf.reset_index().copy()\n",
        "\n",
        "# 2. Convert all geometries to points (centroids for polygons, original points for points)\n",
        "point_geometries = sensitive_locations_gdf.geometry.apply(\n",
        "    lambda geom: geom.centroid if geom.geom_type in ['Polygon', 'MultiPolygon', 'LineString', 'MultiLineString'] else geom)\n",
        "\n",
        "# Replace the original 'geometry' column with the new point geometries\n",
        "sensitive_locations_gdf.geometry = point_geometries\n",
        "\n",
        "sensitive_locations_points = sensitive_locations_gdf[sensitive_locations_gdf.geometry.geom_type == 'Point'].copy()\n",
        "\n",
        "if len(sensitive_locations_gdf) != len(sensitive_locations_points):\n",
        "    print(f\"Warning: {len(sensitive_locations_gdf) - len(sensitive_locations_points)} non-Point geometries were dropped after centroid conversion.\")\n",
        "else:\n",
        "    print(\"All features successfully converted to Point geometries.\")\n",
        "\n",
        "# 3. Reproject to British National Grid (EPSG:27700)\n",
        "target_crs = \"EPSG:27700\"\n",
        "sensitive_locations_bng = sensitive_locations_points.to_crs(target_crs)\n",
        "print(f\"Reprojected CRS: {sensitive_locations_bng.crs}\")\n",
        "\n",
        "# Verify the coordinate ranges (should now be in BNG for Sheffield)\n",
        "print(\"Reprojected Sensitive Locations X range (BNG): \"\n",
        "      f\"{sensitive_locations_bng.geometry.x.min():.2f} to {sensitive_locations_bng.geometry.x.max():.2f}\")\n",
        "print(\"Reprojected Sensitive Locations Y range (BNG): \"\n",
        "      f\"{sensitive_locations_bng.geometry.y.min():.2f} to {sensitive_locations_bng.geometry.y.max():.2f}\")\n",
        "\n",
        "sensitive_locations_bng.to_file(\"sheffield_sensitive_locations_BNG.geojson\")\n",
        "# sensitive_locations_bng.to_csv(\"sheffield_sensitive_locations_BNG.csv\", index=False)\n",
        "\n",
        "# Display a sample of the data\n",
        "print(\"\\nSample of retrieved data (first 5 rows):\")\n",
        "print(sensitive_locations_bng[['name', 'amenity', 'healthcare', 'geometry']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O11p1-9MUVic"
      },
      "source": [
        "check corrodinates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgGVhIy9Gt8g"
      },
      "outputs": [],
      "source": [
        "print(f\"Sensitive Locations X range: {sensitive_locations_bng.geometry.x.min()} to {sensitive_locations_bng.geometry.x.max()}\")\n",
        "print(f\"Sensitive Locations Y range: {sensitive_locations_bng.geometry.y.min()} to {sensitive_locations_bng.geometry.y.max()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEKcSWY5Taaw"
      },
      "outputs": [],
      "source": [
        "# Define the output directory for categorised GeoJSON files\n",
        "output_directory_categories = \"categorised_sensitive_locations_geojson\"\n",
        "os.makedirs(output_directory_categories, exist_ok=True)\n",
        "\n",
        "print(f\"Saving categorized sensitive locations to: {output_directory_categories}\\n\")\n",
        "\n",
        "# Define the categories and their corresponding OSM tag values\n",
        "categories = {\n",
        "    \"hospital\": [\"hospital\"],\n",
        "    \"university\": [\"university\"],\n",
        "    \"school\": [\"school\"],\n",
        "    \"kindergarten\": [\"kindergarten\"],\n",
        "    \"nursing_home\": [\"nursing_home\"], # This covers care homes\n",
        "    \"childcare\": [\"childcare\"]\n",
        "    }\n",
        "\n",
        "# Process each category\n",
        "for category_name, amenity_values in categories.items():\n",
        "    filtered_gdf = sensitive_locations_bng[sensitive_locations_bng['amenity'].isin(amenity_values)].copy()\n",
        "\n",
        "    # Special handling for 'hospital' to also include healthcare=hospital tag\n",
        "    if category_name == \"hospital\":\n",
        "        hospitals_by_healthcare_tag = sensitive_locations_bng[sensitive_locations_bng['healthcare'] == 'hospital'].copy()\n",
        "\n",
        "        # Combine the two sets of hospitals\n",
        "        combined_hospitals_gdf = pd.concat([filtered_gdf, hospitals_by_healthcare_tag])\n",
        "\n",
        "        # Check if 'id' column exists before trying to deduplicate by it\n",
        "        if 'id' in combined_hospitals_gdf.columns:\n",
        "            filtered_gdf = combined_hospitals_gdf.drop_duplicates(subset=['id'], keep='first')\n",
        "            print(f\"    Deduplicated {category_name} using 'id'.\")\n",
        "        else:\n",
        "            filtered_gdf = combined_hospitals_gdf\n",
        "            print(f\"    Warning: 'id' not found for {category_name}. Skipping deduplication.\")\n",
        "\n",
        "\n",
        "    if not filtered_gdf.empty:\n",
        "        # Define the output file path\n",
        "        file_path = os.path.join(output_directory_categories, f\"{category_name}.geojson\")\n",
        "\n",
        "        # Save the filtered GeoDataFrame to a GeoJSON file\n",
        "        try:\n",
        "            filtered_gdf.to_file(file_path, driver=\"GeoJSON\")\n",
        "            print(f\"Saved {len(filtered_gdf)} {category_name} features to {file_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving {category_name} to {file_path}: {e}\")\n",
        "    else:\n",
        "        print(f\"No {category_name} features found to save.\")\n",
        "\n",
        "print(\"\\nCategorization and saving complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WV9tLbzUK_d"
      },
      "outputs": [],
      "source": [
        "# check data type\n",
        "sensitive_locations_bng.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmLmJ1MUDDFL"
      },
      "source": [
        "### Visualisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ex-8zcLHRoj7"
      },
      "outputs": [],
      "source": [
        "# Create folium map\n",
        "m_all_sensitive = folium.Map(location=[53.3810, -1.4701], zoom_start=13)\n",
        "\n",
        "# Add the fixed GeoJson layer\n",
        "folium.GeoJson(\n",
        "    sensitive_locations_bng,\n",
        "    name=\"Sensitive Locations\",\n",
        "    style_function=lambda x: {\n",
        "        'fillColor': '#3186cc',\n",
        "        'color': '#3186cc',\n",
        "        'weight': 1,\n",
        "        'fillOpacity': 0.5\n",
        "    }\n",
        ").add_to(m_all_sensitive)\n",
        "\n",
        "# Show the map\n",
        "m_all_sensitive # it will show all sensitive locations as points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQ_IOTbjHUVQ"
      },
      "outputs": [],
      "source": [
        "# Define the categories and their corresponding OSM tag values\n",
        "categories = {\n",
        "    \"Hospital\": [\"hospital\"],\n",
        "    \"University\": [\"university\"],\n",
        "    \"School\": [\"school\"],\n",
        "    \"Kindergarten\": [\"kindergarten\"],\n",
        "    \"Nursing Home\": [\"nursing_home\"], # This covers care homes\n",
        "    \"Childcare\": [\"childcare\"]\n",
        "}\n",
        "\n",
        "# Define a color for each category for visual distinction on the map\n",
        "category_colors = {\n",
        "    \"Hospital\": \"darkred\",\n",
        "    \"University\": \"darkblue\",\n",
        "    \"School\": \"yellow\",\n",
        "    \"Kindergarten\": \"purple\",\n",
        "    \"Nursing Home\": \"orange\",\n",
        "    \"Childcare\": \"pink\"\n",
        "}\n",
        "\n",
        "sensitive_locations_wgs84 = sensitive_locations_bng.to_crs(epsg=4326)\n",
        "m_sensitive_locations = folium.Map(location=[53.3810, -1.4701], zoom_start=13)\n",
        "\n",
        "# Add the city and ward boundary GeoJSON layers\n",
        "try:\n",
        "    sheffield_city_boundary_wgs84 = gpd.read_file(\"City_Boundary.geojson\").to_crs(epsg=4326)\n",
        "    sheffield_ward_boundary_wgs84 = gpd.read_file(\"Sheffield_Wards.geojson\").to_crs(epsg=4326)\n",
        "\n",
        "    # Add Sheffield City Boundary\n",
        "    folium.GeoJson(\n",
        "        sheffield_city_boundary_wgs84,\n",
        "        name=\"Sheffield City Boundary\",\n",
        "        style_function=lambda x: {\n",
        "            'color': 'black',\n",
        "            'weight': 2,\n",
        "            'fillColor': 'none',\n",
        "            'fillOpacity': 0\n",
        "        }\n",
        "    ).add_to(m_sensitive_locations)\n",
        "\n",
        "    # Add Sheffield Ward Boundary\n",
        "    folium.GeoJson(\n",
        "        sheffield_ward_boundary_wgs84,\n",
        "        name=\"Sheffield Ward Boundary\",\n",
        "        style_function=lambda x: {\n",
        "            'color': 'black',\n",
        "            'weight': 1,\n",
        "            'fillColor': 'none',\n",
        "            'fillOpacity': 0\n",
        "        }\n",
        "    ).add_to(m_sensitive_locations)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading boundary GeoJSON files: {e}\")\n",
        "\n",
        "\n",
        "# Process each category and add a distinct layer\n",
        "for category_name, amenity_values in categories.items():\n",
        "    filtered_gdf = sensitive_locations_wgs84[sensitive_locations_wgs84['amenity'].isin(amenity_values)].copy()\n",
        "\n",
        "    # Special handling for 'hospital' to also include healthcare=hospital tag\n",
        "    if category_name == \"Hospital\":\n",
        "        hospitals_by_healthcare_tag = sensitive_locations_wgs84[sensitive_locations_wgs84['healthcare'] == 'hospital'].copy()\n",
        "        combined_hospitals_gdf = pd.concat([filtered_gdf, hospitals_by_healthcare_tag])\n",
        "\n",
        "        # Check for 'id' column before deduplicating, as per your original code\n",
        "        if 'id' in combined_hospitals_gdf.columns:\n",
        "            filtered_gdf = combined_hospitals_gdf.drop_duplicates(subset=['id'], keep='first')\n",
        "        else:\n",
        "            filtered_gdf = combined_hospitals_gdf\n",
        "\n",
        "    # Use a FeatureGroup to hold the layer, which is useful for grouping layers\n",
        "    feature_group = folium.FeatureGroup(name=category_name)\n",
        "\n",
        "    # Add each point as a CircleMarker to the feature group\n",
        "    for index, row in filtered_gdf.iterrows():\n",
        "        folium.CircleMarker(\n",
        "            location=[row.geometry.y, row.geometry.x],\n",
        "            radius=5, # You can adjust the circle size\n",
        "            color=category_colors.get(category_name, 'gray'), # Use the color from the dictionary\n",
        "            fill=True,\n",
        "            fillColor=category_colors.get(category_name, 'gray'),\n",
        "            fillOpacity=1,\n",
        "            tooltip=f\"<b>Name:</b> {row.get('name', 'N/A')}<br><b>Amenity:</b> {row.get('amenity', 'N/A')}\"\n",
        "        ).add_to(feature_group)\n",
        "\n",
        "    # Add the feature group to the map\n",
        "    feature_group.add_to(m_sensitive_locations)\n",
        "\n",
        "# Add the LayerControl to the map to allow users to toggle layers\n",
        "folium.LayerControl().add_to(m_sensitive_locations)\n",
        "\n",
        "m_sensitive_locations.save(\"sheffield_map_all-sensitive.html\")\n",
        "\n",
        "# Display the map\n",
        "m_sensitive_locations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7D9Uf8ESqbX"
      },
      "source": [
        "## Diffusion tubes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xu-CIm_PtByF"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKaAaSAPSs7o"
      },
      "outputs": [],
      "source": [
        "# Diffusion tubes locations\n",
        "#diffusion = gpd.read_file(\"/content/drive/MyDrive/Diffusion_Tubes.geojson\")\n",
        "diffusion = gpd.read_file(\"/content/Diffusion_Tubes.geojson\")\n",
        "\n",
        "# Add type\n",
        "diffusion['type'] = 'Diffusion_tube'\n",
        "\n",
        "# Set CRS to EPSG:4326: This uses for plotting in lat-long coordinate\n",
        "diffusion_gdf = diffusion.set_crs(epsg=4326, allow_override=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6UzgW0PUHrn"
      },
      "outputs": [],
      "source": [
        "print(diffusion_gdf.head())\n",
        "print(diffusion_gdf.crs)\n",
        "print(diffusion_gdf.geometry.iloc[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpovPUVjs7f3"
      },
      "outputs": [],
      "source": [
        "diffusion_gdf[\"yr2023\"].isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YztIfdhwUIIN"
      },
      "outputs": [],
      "source": [
        "# To check if a coordinate pair likely represents Lat/Lon (EPSG:4326)\n",
        "def looks_like_wgs84(x, y):\n",
        "    \"\"\"\n",
        "    Checks if a given (x, y) coordinate pair falls within typical WGS84 (Lat/Lon) ranges.\n",
        "    Returns True if it looks like WGS84, False otherwise.\n",
        "    \"\"\"\n",
        "    return (-180 <= x <= 180) and (-90 <= y <= 90)\n",
        "\n",
        "print(f\"Checking coordinates for GeoDataFrame with declared CRS: {diffusion_gdf.crs}\\n\")\n",
        "\n",
        "# Iterate through rows and print coordinates that do NOT look like Lat/Lon\n",
        "for i, row in diffusion_gdf.iterrows():\n",
        "    x_coord = row.geometry.x\n",
        "    y_coord = row.geometry.y\n",
        "\n",
        "    if not looks_like_wgs84(x_coord, y_coord):\n",
        "        print(f\"Index {i}: X={x_coord:.6f}, Y={y_coord:.6f} (Likely NOT Lat/Lon)\")\n",
        "    else:\n",
        "        # Optional: Print those that DO look like Lat/Lon if you want to see them\n",
        "        # print(f\"Index {i}: X={x_coord:.6f}, Y={y_coord:.6f} (Likely Lat/Lon)\")\n",
        "        pass # Do nothing if it looks like Lat/Lon (as per your request)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUD9dMUQVGy7"
      },
      "outputs": [],
      "source": [
        "# Center the map on Sheffield's coordinates (Latitude: 53.3810, Longitude: -1.4701)\n",
        "sheffield_center = [53.3810, -1.4701]\n",
        "\n",
        "# Create a Folium map centered on Sheffield\n",
        "m_diffusion_tubes = folium.Map(location=sheffield_center, zoom_start=13)\n",
        "\n",
        "# Plot each diffusion tube location\n",
        "for _, row in diffusion_gdf.iterrows():\n",
        "    folium.Marker(\n",
        "        location=[row.geometry.y, row.geometry.x],\n",
        "        popup=\"Diffusion Tube\"\n",
        "        ).add_to(m_diffusion_tubes)\n",
        "\n",
        "# Display the map\n",
        "m_diffusion_tubes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhHcoUy8Zb6d"
      },
      "outputs": [],
      "source": [
        "diffusion_bng = diffusion_gdf.to_crs(epsg=27700)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w58n-qckZvD5"
      },
      "source": [
        "## Find distance threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wk5rqdyKZQT3"
      },
      "outputs": [],
      "source": [
        "# Check CRS\n",
        "print(diffusion_bng.crs)\n",
        "print(sensitive_locations_bng.crs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLTH89CgaZLy"
      },
      "source": [
        "### Diffusion tubes to sensitive locations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsxWRlBWZzlH"
      },
      "outputs": [],
      "source": [
        "# Compute nearest distance from each diffusion tube to a sensitive location.\n",
        "distance_DT = diffusion_bng.geometry.apply(\n",
        "    lambda tube: sensitive_locations_bng.distance(tube).min()\n",
        "    )\n",
        "\n",
        "# Summary\n",
        "print(distance_DT.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCatx-mWxT6M"
      },
      "outputs": [],
      "source": [
        "# Perform spatial join: find each diffusion tube to nearest sensitive location\n",
        "dt_with_nearest_sl = gpd.sjoin_nearest(\n",
        "    diffusion_bng,                    # LEFT GeoDataFrame: diffusion tubes\n",
        "    sensitive_locations_bng,          # RIGHT GeoDataFrame: sensitive locations\n",
        "    how='left',\n",
        "    max_distance=float('inf'),        # Consider all sensitive locations\n",
        "    distance_col='distance_to_nearest_sensitive_location'\n",
        ")\n",
        "\n",
        "# Remove duplicate diffusion tubes if necessary (e.g., based on 'id' or index)\n",
        "if 'id' in dt_with_nearest_sl.columns:\n",
        "    diffusion_with_nearest_sl = dt_with_nearest_sl.drop_duplicates(subset=['id'], keep='first').copy()\n",
        "    print(\"Deduplicated diffusion tubes using 'id' column.\")\n",
        "else:\n",
        "    diffusion_with_nearest_sl = dt_with_nearest_sl.drop_duplicates(keep='first').copy()\n",
        "    print(\"Warning: 'id' not found. Deduplicated diffusion tubes using index.\")\n",
        "\n",
        "# Compute Q1, Q3, and IQR for distances\n",
        "Q1 = diffusion_with_nearest_sl['distance_to_nearest_sensitive_location'].quantile(0.25)\n",
        "Q3 = diffusion_with_nearest_sl['distance_to_nearest_sensitive_location'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Define upper bound for outlier detection\n",
        "upper_bound_outlier = Q3 + 1.5 * IQR\n",
        "\n",
        "# Output summary stats\n",
        "print(f\"\\nQ1 (25th percentile): {Q1:.2f} m\")\n",
        "print(f\"Q3 (75th percentile): {Q3:.2f} m\")\n",
        "print(f\"IQR (Interquartile Range): {IQR:.2f} m\")\n",
        "print(f\"Upper bound for outlier detection (Q3 + 1.5 * IQR): {upper_bound_outlier:.2f} m\")\n",
        "\n",
        "# Identify outlier diffusion tubes (those far from any sensitive location)\n",
        "outlier_diffusion_tubes = diffusion_with_nearest_sl[\n",
        "    diffusion_with_nearest_sl['distance_to_nearest_sensitive_location'] > upper_bound_outlier\n",
        "].copy()\n",
        "\n",
        "print(f\"\\nNumber of outlier diffusion tubes: {len(outlier_diffusion_tubes)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28WrYBYCadbV"
      },
      "source": [
        "### Sensitive locations to diffusion tubes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XohZ9zB1aCfV"
      },
      "outputs": [],
      "source": [
        "# Compute nearest distance from each sensitive location to diffusion tubes.\n",
        "distance_SL = sensitive_locations_bng.geometry.apply(\n",
        "    lambda tube: diffusion_bng.distance(tube).min())\n",
        "\n",
        "# Summary\n",
        "print(distance_SL.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZRZFjZLdZiz"
      },
      "outputs": [],
      "source": [
        "print(f\"Sensitive Locations: {len(sensitive_locations_bng)} features\")\n",
        "print(f\"Diffusion Tubes: {len(diffusion_bng)} features\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzbsRcgo42rt"
      },
      "outputs": [],
      "source": [
        "# Calculate Q1 and Q3\n",
        "q1_distance = distance_SL.quantile(0.25)\n",
        "q3_distance = distance_SL.quantile(0.75)\n",
        "\n",
        "# Calculate Interquartile Range (IQR)\n",
        "iqr_distance = q3_distance - q1_distance\n",
        "\n",
        "# Calculate Upper Bound (for outlier detection, using 1.5 * IQR rule)\n",
        "upper_bound_distance = q3_distance + 1.5 * iqr_distance\n",
        "\n",
        "print(f\"Interquartile Range (IQR) of distances: {iqr_distance:.2f} meters\")\n",
        "print(f\"Upper Bound for outlier detection: {upper_bound_distance:.2f} meters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LVd0jOCrx-T"
      },
      "source": [
        "Calculate distance statistics for sensitive locations to ***nearest*** diffusion tube"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVyDyXOziWbX"
      },
      "outputs": [],
      "source": [
        "# Calculate the distance from each sensitive location to its nearest diffusion tube\n",
        "\n",
        "sl_with_nearest_dt = gpd.sjoin_nearest(\n",
        "    sensitive_locations_bng, # LEFT: sensitive location\n",
        "    diffusion_bng, # RIGHT: diffusion tubes\n",
        "    how='left',\n",
        "    max_distance=float('inf'), # Consider all diffusion tubes\n",
        "    distance_col='distance_to_nearest_diffusion_tube'\n",
        "    )\n",
        "\n",
        "# Remove duplicate entries\n",
        "if 'id' in sl_with_nearest_dt.columns:\n",
        "    sensitive_locations_with_nearest_dt = sl_with_nearest_dt.drop_duplicates(subset=['id'], keep='first').copy()\n",
        "    print(\"Deduplicated sensitive locations using 'osmid'.\")\n",
        "else:\n",
        "    if 'id' in sl_with_nearest_dt.columns:\n",
        "        sensitive_locations_with_nearest_dt = sl_with_nearest_dt.drop_duplicates(subset=['id'], keep='first').copy()\n",
        "        print(\"Deduplicated sensitive locations using 'id' column.\")\n",
        "    else:\n",
        "        # Option 2: Fallback to the original index of the LEFT dataframe (the sensitive locations)\n",
        "        sensitive_locations_with_nearest_dt = sl_with_nearest_dt.drop_duplicates(keep='first').copy()\n",
        "        print(\"Warning: 'osmid' or 'id' not found. Deduplicated sensitive locations using the GeoDataFrame's index.\")\n",
        "        # sl_with_nearest_dt.sort_values(by='distance_to_nearest_diffusion_tube', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EineEUgGjJTm"
      },
      "outputs": [],
      "source": [
        "# Check the descriptive statistics for this new distance column\n",
        "print(\"\\nDistance statistics for Sensitive Locations to Nearest Diffusion Tube:\")\n",
        "print(sl_with_nearest_dt['distance_to_nearest_diffusion_tube'].describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMR3ednysYYA"
      },
      "source": [
        "Define outliers using the IQR method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYkyVxcErn9L"
      },
      "outputs": [],
      "source": [
        "# Calculate Q1, Q3, and IQR\n",
        "Q1 = sensitive_locations_with_nearest_dt['distance_to_nearest_diffusion_tube'].quantile(0.25)\n",
        "Q3 = sensitive_locations_with_nearest_dt['distance_to_nearest_diffusion_tube'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Define the upper bound for outliers (using 1.5 * IQR rule)\n",
        "# Outliers are typically values greater than Q3 + 1.5 * IQR\n",
        "upper_bound_outlier = Q3 + 1.5 * IQR\n",
        "\n",
        "print(f\"\\nQ1 (25th percentile): {Q1:.2f} m\")\n",
        "print(f\"Q3 (75th percentile): {Q3:.2f} m\")\n",
        "print(f\"IQR (Interquartile Range): {IQR:.2f} m\")\n",
        "print(f\"Upper bound for outlier detection (Q3 + 1.5 * IQR): {upper_bound_outlier:.2f} m\")\n",
        "\n",
        "# Identify the outlier sensitive locations\n",
        "outlier_sensitive_locations = sensitive_locations_with_nearest_dt[\n",
        "    sensitive_locations_with_nearest_dt['distance_to_nearest_diffusion_tube'] > upper_bound_outlier].copy()\n",
        "\n",
        "print(f\"\\nNumber of identified outlier sensitive locations: {len(outlier_sensitive_locations)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EapRLXeGUrpT"
      },
      "outputs": [],
      "source": [
        "outlier_sensitive_locations[['amenity', 'name', 'distance_to_nearest_diffusion_tube', 'geometry']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxdwJKfxsvYf"
      },
      "outputs": [],
      "source": [
        "if not outlier_sensitive_locations.empty:\n",
        "    print(\"\\nOutlier Sensitive Locations breakdown by Amenity type:\")\n",
        "    print(outlier_sensitive_locations['amenity'].value_counts())\n",
        "    print(\"\\nDetails of Outlier Sensitive Locations (first 5 rows):\")\n",
        "    print(outlier_sensitive_locations[['amenity', 'name', 'distance_to_nearest_diffusion_tube', 'geometry']].head(20))\n",
        "else:\n",
        "    print(\"\\nNo outlier sensitive locations identified to break down by amenity type.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtpWikLv3WBi"
      },
      "source": [
        "### Visualising the outliers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhp0AS7ZuE_E"
      },
      "source": [
        "\n",
        "This will help to answer:\n",
        "- Which specific locations are poorly covered.\n",
        "- What types of facilities (schools, hospitals, etc.) are commonly found among these outliers.\n",
        "- Where geographically these monitoring gaps are located."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMGBnQiRsycC"
      },
      "outputs": [],
      "source": [
        "# Reproject GeoDataFrames to WGS84 (EPSG:4326) for Folium\n",
        "sensitive_locations_bng_wgs84 = sensitive_locations_bng.to_crs(epsg=4326)\n",
        "outlier_sensitive_locations_wgs84 = outlier_sensitive_locations.to_crs(epsg=4326)\n",
        "\n",
        "# Center the map on Sheffield (Latitude, Longitude)\n",
        "sheffield_center = [53.3810, -1.4701]\n",
        "m_outliers = folium.Map(location=sheffield_center, zoom_start=12)\n",
        "\n",
        "# Add Diffusion Tubes (blue markers)\n",
        "diffusion_tube_group = folium.FeatureGroup(name=\"Diffusion Tubes\").add_to(m_outliers)\n",
        "for _, row in diffusion_gdf.iterrows():\n",
        "    folium.Marker(location=[row.geometry.y, row.geometry.x],\n",
        "                  popup=\"Diffusion Tube\").add_to(diffusion_tube_group)\n",
        "\n",
        "# Add ALL Sensitive Locations (green circles)\n",
        "all_sensitive_group = folium.FeatureGroup(name=\"All Sensitive Locations\").add_to(m_outliers)\n",
        "for _, row in sensitive_locations_bng_wgs84.iterrows():\n",
        "    folium.CircleMarker(\n",
        "        location=[row.geometry.y, row.geometry.x],\n",
        "        radius=5, # pixels\n",
        "        color='green',\n",
        "        fill=True,\n",
        "        fill_color='lightgreen',\n",
        "        fill_opacity=0.7,\n",
        "        popup=f\"Sensitive Location: {row.get('amenity', 'N/A')}<br>Name: {row.get('name', 'N/A')}\"\n",
        "    ).add_to(all_sensitive_group)\n",
        "\n",
        "# Add Outlier Sensitive Locations (larger red circles)\n",
        "outlier_group = folium.FeatureGroup(name=\"Outlier Sensitive Locations\").add_to(m_outliers)\n",
        "if not outlier_sensitive_locations_wgs84.empty:\n",
        "    for _, row in outlier_sensitive_locations_wgs84.iterrows():\n",
        "        folium.CircleMarker(\n",
        "            location=[row.geometry.y, row.geometry.x],\n",
        "            radius=8, # Larger to stand out\n",
        "            color='red',\n",
        "            fill=True,\n",
        "            fill_color='darkred',\n",
        "            fill_opacity=0.9,\n",
        "            popup=(f\"OUTLIER! Type: {row.get('amenity', 'N/A')}<br>\"\n",
        "                   f\"Name: {row.get('name', 'N/A')}<br>\"\n",
        "                   f\"Distance to Tube: {row['distance_to_nearest_diffusion_tube']:.2f} m\")\n",
        "        ).add_to(outlier_group)\n",
        "else:\n",
        "    print(\"No outliers to plot on the map.\")\n",
        "\n",
        "# Add a Layer Control to toggle layers on/off (very useful for interactive maps)\n",
        "folium.LayerControl().add_to(m_outliers)\n",
        "\n",
        "m_outliers.save(\"sheffield_map.html\")\n",
        "\n",
        "# Display the map (will render in a Jupyter Notebook/environment)\n",
        "m_outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_bqqFzPJwOu"
      },
      "outputs": [],
      "source": [
        "import folium\n",
        "import geopandas as gpd\n",
        "\n",
        "# Load the GeoJSON file for the Sheffield City boundary\n",
        "sheffield_city_boundary = gpd.read_file(\"City_Boundary.geojson\")\n",
        "\n",
        "# Load the GeoJSON file for the Sheffield Wards boundary\n",
        "sheffield_ward_boundary = gpd.read_file(\"Sheffield_Wards.geojson\")\n",
        "\n",
        "# Reproject GeoDataFrames to WGS84 (EPSG:4326) for Folium\n",
        "sensitive_locations_bng_wgs84 = sensitive_locations_bng.to_crs(epsg=4326)\n",
        "outlier_sensitive_locations_wgs84 = outlier_sensitive_locations.to_crs(epsg=4326)\n",
        "sheffield_city_boundary_wgs84 = sheffield_city_boundary.to_crs(epsg=4326)\n",
        "sheffield_ward_boundary_wgs84 = sheffield_ward_boundary.to_crs(epsg=4326)\n",
        "\n",
        "# Center the map on Sheffield (Latitude, Longitude)\n",
        "sheffield_center = [53.3810, -1.4701]\n",
        "m_outlier_2 = folium.Map(location=sheffield_center, zoom_start=12)\n",
        "\n",
        "# Add Sheffield City Boundary\n",
        "folium.GeoJson(\n",
        "    sheffield_city_boundary_wgs84,\n",
        "    name=\"Sheffield City Boundary\",\n",
        "    style_function=lambda x: {\n",
        "        'color': 'black',\n",
        "        'weight': 2,\n",
        "        'fillColor': 'none',\n",
        "        'fillOpacity': 0\n",
        "    }\n",
        ").add_to(m_outlier_2)\n",
        "\n",
        "# Add Sheffield Ward Boundary\n",
        "folium.GeoJson(\n",
        "    sheffield_ward_boundary_wgs84,\n",
        "    name=\"Sheffield Ward Boundary\",\n",
        "    style_function=lambda x: {\n",
        "        'color': 'black',\n",
        "        'weight': 1,\n",
        "        'fillColor': 'none',\n",
        "        'fillOpacity': 0\n",
        "    }\n",
        ").add_to(m_outlier_2)\n",
        "\n",
        "# Add Diffusion Tubes (blue circles)\n",
        "diffusion_tube_group = folium.FeatureGroup(name=\"Diffusion Tubes\").add_to(m_outlier_2)\n",
        "for _, row in diffusion_gdf.iterrows():\n",
        "    folium.CircleMarker(\n",
        "        location=[row.geometry.y, row.geometry.x],\n",
        "        radius=5,  # pixels\n",
        "        color='blue',\n",
        "        fill=True,\n",
        "        fill_color='blue',\n",
        "        fill_opacity=0.7,\n",
        "        popup=\"Diffusion Tube\"\n",
        "    ).add_to(diffusion_tube_group)\n",
        "\n",
        "# Add ALL Sensitive Locations (green circles)\n",
        "all_sensitive_group = folium.FeatureGroup(name=\"All Sensitive Locations\").add_to(m_outlier_2)\n",
        "for _, row in sensitive_locations_bng_wgs84.iterrows():\n",
        "    folium.CircleMarker(\n",
        "        location=[row.geometry.y, row.geometry.x],\n",
        "        radius=5,  # pixels\n",
        "        color='green',\n",
        "        fill=True,\n",
        "        fill_color='lightgreen',\n",
        "        fill_opacity=0.7,\n",
        "        popup=f\"Sensitive Location: {row.get('amenity', 'N/A')}<br>Name: {row.get('name', 'N/A')}\"\n",
        "    ).add_to(all_sensitive_group)\n",
        "\n",
        "# Add Outlier Sensitive Locations (larger red circles)\n",
        "outlier_group = folium.FeatureGroup(name=\"Outlier Sensitive Locations\").add_to(m_outlier_2)\n",
        "if not outlier_sensitive_locations_wgs84.empty:\n",
        "    for _, row in outlier_sensitive_locations_wgs84.iterrows():\n",
        "        folium.CircleMarker(\n",
        "            location=[row.geometry.y, row.geometry.x],\n",
        "            radius=8,  # Larger to stand out\n",
        "            color='red',\n",
        "            fill=True,\n",
        "            fill_color='darkred',\n",
        "            fill_opacity=0.9,\n",
        "            popup=(f\"OUTLIER! Type: {row.get('amenity', 'N/A')}<br>\"\n",
        "                   f\"Name: {row.get('name', 'N/A')}<br>\"\n",
        "                   f\"Distance to Tube: {row['distance_to_nearest_diffusion_tube']:.2f} m\")\n",
        "        ).add_to(outlier_group)\n",
        "else:\n",
        "    print(\"No outliers to plot on the map.\")\n",
        "\n",
        "# Add a Layer Control to toggle layers on/off\n",
        "folium.LayerControl().add_to(m_outlier_2)\n",
        "\n",
        "# Save the map to an HTML file\n",
        "m_outlier_2.save(\"sheffield_outlier_with_wards.html\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJEb521lO0Mt"
      },
      "outputs": [],
      "source": [
        "m_outlier_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRE_9utTvrNQ"
      },
      "outputs": [],
      "source": [
        "# Total count per amenity type\n",
        "total_counts = sensitive_locations_with_nearest_dt['amenity'].value_counts()\n",
        "print(\"\\nTotal sensitive locations per amenity type:\\n\", total_counts)\n",
        "\n",
        "# Outlier count per amenity type\n",
        "outlier_counts = outlier_sensitive_locations['amenity'].value_counts()\n",
        "print(\"\\nOutlier sensitive locations per amenity type:\\n\", outlier_counts)\n",
        "\n",
        "# Percentage of each amenity type that are outliers\n",
        "percentage_outliers = (outlier_counts / total_counts * 100).fillna(0).sort_values(ascending=False)\n",
        "print(\"\\nPercentage of each amenity type that are outliers:\\n\", percentage_outliers.round(2))\n",
        "\n",
        "# You might also want to look at average/median distance per amenity type for ALL locations\n",
        "print(\"\\nAverage distance to nearest tube per amenity type (all locations):\\n\",\n",
        "      sensitive_locations_with_nearest_dt.groupby('amenity')['distance_to_nearest_diffusion_tube'].mean().round(2).sort_values(ascending=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nSPZubcF6Fp"
      },
      "source": [
        "## Other locations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc_0vuTwEqGZ"
      },
      "source": [
        "Retrieve data of other locations such as industrial area, park, and main road."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edIZhJ-ziHrL"
      },
      "outputs": [],
      "source": [
        "diffusion_bng.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyMXptjlwk8s"
      },
      "outputs": [],
      "source": [
        "place_name = \"Sheffield, UK\"\n",
        "target_crs = \"EPSG:27700\" # British National Grid\n",
        "\n",
        "output_dir_colocation = \"colocation_data\"\n",
        "os.makedirs(output_dir_colocation, exist_ok=True)\n",
        "\n",
        "print(\"Starting co-location pattern mining setup...\")\n",
        "print(\"\\nUsing existing sensitive_locations_bng and diffusion_gdf_bng.\")\n",
        "print(f\"  Sensitive Locations: {len(sensitive_locations_bng)} features (CRS: {sensitive_locations_bng.crs})\")\n",
        "print(f\"  Diffusion Tubes: {len(diffusion_bng)} features (CRS: {diffusion_bng.crs})\")\n",
        "\n",
        "# Ensure feature_type is set for these existing datasets for consistency\n",
        "if 'feature_type' not in sensitive_locations_bng.columns:\n",
        "    # This assumes 'amenity' or 'healthcare' exists for sensitive locations\n",
        "    sensitive_locations_bng['feature_type'] = sensitive_locations_bng['amenity'].fillna(sensitive_locations_bng['healthcare']).copy()\n",
        "    sensitive_locations_bng = sensitive_locations_bng[~sensitive_locations_bng['feature_type'].isna()].copy()\n",
        "if 'feature_type' not in diffusion_bng.columns:\n",
        "    diffusion_bng['feature_type'] = 'diffusion_tube'\n",
        "\n",
        "# 2. Retrieve Other Location Types\n",
        "print(\"\\nRetrieving other location types (Industrial, Parks)...\")\n",
        "\n",
        "other_tags = {\"landuse\": [\"industrial\"],\n",
        "              \"leisure\": [\"park\", \"recreation_ground\"]}\n",
        "other_locations_gdf_raw = ox.features.features_from_place(place_name, other_tags)\n",
        "other_locations_gdf_raw = other_locations_gdf_raw.reset_index().copy()\n",
        "\n",
        "# Convert all geometries to points (centroids)\n",
        "other_locations_gdf_raw['geometry'] = other_locations_gdf_raw.geometry.apply(\n",
        "    lambda geom: geom.centroid if geom.geom_type in ['Polygon', 'MultiPolygon', 'LineString', 'MultiLineString', 'Point'] else None)\n",
        "other_locations_gdf_raw = other_locations_gdf_raw.dropna(subset=['geometry']).copy()\n",
        "\n",
        "# Assign feature_type based on original tags\n",
        "other_locations_gdf_raw['feature_type'] = None\n",
        "other_locations_gdf_raw.loc[other_locations_gdf_raw['landuse'] == 'industrial', 'feature_type'] = 'industrial_area'\n",
        "other_locations_gdf_raw.loc[other_locations_gdf_raw['leisure'] == 'park', 'feature_type'] = 'park'\n",
        "other_locations_gdf_raw.loc[other_locations_gdf_raw['leisure'] == 'recreation_ground', 'feature_type'] = 'park'\n",
        "\n",
        "other_locations_gdf_points = other_locations_gdf_raw[\n",
        "    ~other_locations_gdf_raw['feature_type'].isna()].copy()\n",
        "other_locations_gdf_points = other_locations_gdf_points.to_crs(target_crs)\n",
        "print(f\"  Processed {len(other_locations_gdf_points)} other locations (industrial, parks).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xs-jeoHUGJ8w"
      },
      "outputs": [],
      "source": [
        "# Define target_crs and output_dir_colocation if not already defined\n",
        "target_crs = \"EPSG:27700\"\n",
        "output_dir_colocation = \"colocation_data\"\n",
        "os.makedirs(output_dir_colocation, exist_ok=True) # Ensure the directory exists\n",
        "\n",
        "\n",
        "print(\"\\nCombining all processed features...\")\n",
        "\n",
        "# --- Add a generic 'uid' for consistent tracking across all data sources ---\n",
        "\n",
        "# 1. Sensitive Locations\n",
        "sensitive_uid_col_found = None\n",
        "if 'id' in sensitive_locations_bng.columns:\n",
        "    sensitive_uid_col_found = 'id'\n",
        "elif 'osmid' in sensitive_locations_bng.columns:\n",
        "    sensitive_uid_col_found = 'osmid'\n",
        "\n",
        "if sensitive_uid_col_found:\n",
        "    sensitive_locations_bng['uid'] = sensitive_locations_bng[sensitive_uid_col_found].astype(str) + '_sl'\n",
        "    print(f\"  Sensitive locations UID based on '{sensitive_uid_col_found}'.\")\n",
        "else:\n",
        "    sensitive_locations_bng['uid'] = 'sl_' + sensitive_locations_bng.index.astype(str)\n",
        "    print(\"  Warning: Neither 'id' nor 'osmid' found in sensitive_locations_bng. UID based on index.\")\n",
        "\n",
        "\n",
        "# 2. Diffusion Tubes\n",
        "diffusion_uid_col_found = None\n",
        "# Check for 'objectid' first as per your data\n",
        "if 'objectid' in diffusion_bng.columns:\n",
        "    diffusion_uid_col_found = 'objectid'\n",
        "elif 'id' in diffusion_bng.columns: # Fallback to 'id'\n",
        "    diffusion_uid_col_found = 'id'\n",
        "elif 'osmid' in diffusion_bng.columns: # Fallback to 'osmid'\n",
        "    diffusion_uid_col_found = 'osmid'\n",
        "\n",
        "if diffusion_uid_col_found:\n",
        "    diffusion_bng['uid'] = diffusion_bng[diffusion_uid_col_found].astype(str) + '_dt'\n",
        "    print(f\"  Diffusion tubes UID based on '{diffusion_uid_col_found}'.\")\n",
        "else:\n",
        "    diffusion_bng['uid'] = 'dt_' + diffusion_bng.index.astype(str)\n",
        "    print(\"  Warning: Neither 'objectid', 'id', nor 'osmid' found in diffusion_gdf_points. UID based on index.\")\n",
        "\n",
        "\n",
        "# 3. Other Locations\n",
        "other_uid_col_found = None\n",
        "if 'id' in other_locations_gdf_points.columns:\n",
        "    other_uid_col_found = 'id'\n",
        "elif 'osmid' in other_locations_gdf_points.columns:\n",
        "    other_uid_col_found = 'osmid'\n",
        "\n",
        "if other_uid_col_found:\n",
        "    other_locations_gdf_points['uid'] = other_locations_gdf_points[other_uid_col_found].astype(str) + '_other'\n",
        "    print(f\"  Other locations UID based on '{other_uid_col_found}'.\")\n",
        "else:\n",
        "    other_locations_gdf_points['uid'] = 'other_' + other_locations_gdf_points.index.astype(str)\n",
        "    print(\"  Warning: Neither 'id' nor 'osmid' found in other_locations_gdf_points. UID based on index.\")\n",
        "\n",
        "\n",
        "# --- Select only the relevant columns for combination ---\n",
        "cols_to_keep = ['uid', 'feature_type', 'geometry']\n",
        "\n",
        "all_features_gdf = pd.concat([\n",
        "    sensitive_locations_bng[cols_to_keep],\n",
        "    diffusion_bng[cols_to_keep], # Use diffusion_gdf_points here as it's the point data\n",
        "    other_locations_gdf_points[cols_to_keep]\n",
        "]).reset_index(drop=True)\n",
        "\n",
        "# Ensure the combined GeoDataFrame has the correct CRS\n",
        "all_features_gdf.crs = target_crs\n",
        "\n",
        "print(f\"\\nTotal features combined: {len(all_features_gdf)}\")\n",
        "print(\"Combined Feature Types Distribution:\")\n",
        "print(all_features_gdf['feature_type'].value_counts())\n",
        "print(f\"Combined GeoDataFrame CRS: {all_features_gdf.crs}\")\n",
        "\n",
        "# Save the combined data for future use\n",
        "all_features_gdf.to_file(os.path.join(output_dir_colocation, \"all_spatial_features_bng.geojson\"), driver=\"GeoJSON\")\n",
        "print(f\"All features saved to {os.path.join(output_dir_colocation, 'all_spatial_features_bng.geojson')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kz6FswzakKj1"
      },
      "outputs": [],
      "source": [
        "print(f\"Sensitive Locations X range: {all_features_gdf.geometry.x.min()} to {all_features_gdf.geometry.x.max()}\")\n",
        "print(f\"Sensitive Locations Y range: {all_features_gdf.geometry.y.min()} to {all_features_gdf.geometry.y.max()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uyiiNP6DOAK"
      },
      "source": [
        "## Co-location pattern mining"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nuxr_6eYHScW"
      },
      "outputs": [],
      "source": [
        "# --- Configuration for Co-location Mining ---\n",
        "distance_threshold_m = [200, 300, 400, 500, 600]\n",
        "pi_thresholds = [0.01, 0.05, 0.1, 0.2, 0.5]\n",
        "pr_thresholds = [0]\n",
        "\n",
        "print(f\"\\nCo-location mining with distance threshold: {distance_threshold_m} meters\")\n",
        "print(f\"Min Participation Index (PI): {pi_thresholds}\")\n",
        "print(f\"Min Participation Ratio (PR): {pr_thresholds}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M48oSqkB6a_n"
      },
      "outputs": [],
      "source": [
        "# Define path for saving files\n",
        "output_dir_colocation = \"colocation_data\"\n",
        "os.makedirs(output_dir_colocation, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "De8vUw_26gPW"
      },
      "outputs": [],
      "source": [
        "# Loads and processes all_features_gdf\n",
        "try:\n",
        "    if 'all_features_gdf' not in locals() and 'all_features_gdf' not in globals():\n",
        "        print(\"`all_features_gdf` not found. Attempting to load from 'all_spatial_features_bng.geojson'...\")\n",
        "\n",
        "        output_dir_colocation = \"colocation_data\"\n",
        "        geojson_path = os.path.join(output_dir_colocation, \"all_spatial_features_bng.geojson\")\n",
        "\n",
        "        if os.path.exists(geojson_path):\n",
        "            all_features_gdf = gpd.read_file(geojson_path)\n",
        "            target_crs = \"EPSG:27700\"\n",
        "            all_features_gdf.crs = target_crs\n",
        "            print(f\"Loaded {len(all_features_gdf)} features from {geojson_path}.\")\n",
        "        else:\n",
        "            raise FileNotFoundError(f\"Combined GeoJSON not found at: {geojson_path}. Please run data prep first or provide actual all_features_gdf.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading all_features_gdf: {e}\")\n",
        "    print(\"Please ensure `all_features_gdf` is properly loaded/defined before this step.\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40D1viuO6jID"
      },
      "outputs": [],
      "source": [
        "# Ensure geometries are points and CRS is correct before cKDTree\n",
        "if all_features_gdf.crs != target_crs:\n",
        "    print(f\"WARNING: CRS is {all_features_gdf.crs}, expected {target_crs}. Reprojecting...\")\n",
        "    all_features_gdf = all_features_gdf.to_crs(target_crs)\n",
        "    print(f\"Reprojected to: {all_features_gdf.crs}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8MaUUrfU6llj"
      },
      "outputs": [],
      "source": [
        "# Convert non-point geometries to centroids if necessary, and filter to only points\n",
        "geom_types = all_features_gdf.geometry.geom_type.unique()\n",
        "if 'Point' not in geom_types or len(geom_types) > 1:\n",
        "    print(\"Converting non-Point geometries to centroids for cKDTree compatibility...\")\n",
        "    all_features_gdf['geometry'] = all_features_gdf.geometry.apply(\n",
        "        lambda geom: geom.centroid if geom.geom_type != 'Point' else geom)\n",
        "\n",
        "    # Filter out any rows where centroid conversion might have failed or resulted in non-points\n",
        "    all_features_gdf = all_features_gdf[all_features_gdf.geometry.geom_type == 'Point'].copy()\n",
        "    print(f\"Geometry Types after conversion and filtering: {all_features_gdf.geometry.geom_type.unique()}\")\n",
        "    if all_features_gdf.empty:\n",
        "        raise ValueError(\"No valid point geometries remaining after processing for cKDTree. Cannot proceed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6uLqmBfXyB7"
      },
      "source": [
        "### Extract coordinates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZ_ec7DvHZk2"
      },
      "outputs": [],
      "source": [
        "# Extract coordinates for cKDTree\n",
        "coords = np.array([(p.x, p.y) for p in all_features_gdf.geometry])\n",
        "tree = cKDTree(coords)\n",
        "\n",
        "# Get unique feature types\n",
        "unique_feature_types = all_features_gdf['feature_type'].unique().tolist()\n",
        "print(f\"\\nUnique feature types for mining: {unique_feature_types}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8ZbIA4ZIIPB"
      },
      "source": [
        "### Defind neighbouring pairs (instance-level) function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-RfxkOqH06C"
      },
      "outputs": [],
      "source": [
        "def build_neighborhood_graph(current_distance_threshold_m, all_features_gdf, coords, tree):\n",
        "    \"\"\"\n",
        "    Builds the instance-level neighborhood graph based on a distance threshold.\n",
        "\n",
        "    Args:\n",
        "        current_distance_threshold_m (float): The current distance threshold in meters.\n",
        "        all_features_gdf (gpd.GeoDataFrame): GeoDataFrame containing all feature instances.\n",
        "        coords (np.array): Nx2 array of coordinates for KDTree.\n",
        "        tree (cKDTree): KDTree built from the coordinates.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary where keys are instance UIDs and values are\n",
        "              lists of (neighbor_uid, neighbor_type) tuples.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Building Neighborhood Graph for Distance Threshold: {current_distance_threshold_m}m ---\")\n",
        "    neighbors_map = {}\n",
        "    idx_to_uid_type = all_features_gdf[['uid', 'feature_type']].to_dict('index')\n",
        "\n",
        "    for i in range(len(all_features_gdf)):\n",
        "        current_uid = all_features_gdf.iloc[i]['uid']\n",
        "        # Query neighbors for the point at current_idx in the coords array\n",
        "        neighbor_indices = tree.query_ball_point(coords[i], current_distance_threshold_m)\n",
        "\n",
        "        current_instance_neighbors = []\n",
        "        for neighbor_idx in neighbor_indices:\n",
        "            if neighbor_idx != i: # Don't count self as neighbor\n",
        "                neighbor_data = idx_to_uid_type.get(neighbor_idx)\n",
        "                if neighbor_data:\n",
        "                    neighbor_uid = neighbor_data['uid']\n",
        "                    neighbor_type = neighbor_data['feature_type']\n",
        "                    current_instance_neighbors.append((neighbor_uid, neighbor_type))\n",
        "        neighbors_map[current_uid] = current_instance_neighbors\n",
        "\n",
        "    print(f\"Built neighborhood graph (instance-level) for {current_distance_threshold_m}m. Found {len(neighbors_map)} instances with neighbors.\")\n",
        "    return neighbors_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ND9ykSd6YALL"
      },
      "source": [
        "### Define PR/PI function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h__me733WmlZ"
      },
      "outputs": [],
      "source": [
        "# Function to calculate PI/PR for a given pattern\n",
        "def calculate_pattern_prevalence(pattern_types, neighbors_map, all_features_gdf):\n",
        "    \"\"\"\n",
        "    Calculates the Participation Index (PI) and Participation Ratio (PR) for a given pattern.\n",
        "    Assumes a star-based co-location model: an instance participates if it's a central\n",
        "    node that has neighbors of ALL other types in the pattern.\n",
        "    \"\"\"\n",
        "    pattern_pi_values = []\n",
        "    feature_type_counts = all_features_gdf['feature_type'].value_counts().to_dict()\n",
        "\n",
        "    for f_type in pattern_types:\n",
        "        total_instances_of_type = feature_type_counts.get(f_type, 0)\n",
        "        if total_instances_of_type == 0:\n",
        "            pattern_pi_values.append(0.0)\n",
        "            continue\n",
        "\n",
        "        participating_count = 0\n",
        "        instances_of_f_type = all_features_gdf[all_features_gdf['feature_type'] == f_type]\n",
        "\n",
        "        for _, instance_row in instances_of_f_type.iterrows():\n",
        "            instance_uid = instance_row['uid']\n",
        "\n",
        "            neighbors_of_instance = {n_type for _, n_type in neighbors_map.get(instance_uid, [])}\n",
        "            other_types_in_pattern = [t for t in pattern_types if t != f_type]\n",
        "\n",
        "            if all(ot in neighbors_of_instance for ot in other_types_in_pattern):\n",
        "                participating_count += 1\n",
        "\n",
        "        pattern_pi_values.append(participating_count / total_instances_of_type)\n",
        "\n",
        "    pattern_pi = min(pattern_pi_values) if pattern_pi_values else 0.0\n",
        "    pattern_pr = sum(pattern_pi_values) / len(pattern_pi_values)\n",
        "\n",
        "    return pattern_pi, pattern_pr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gpsw7D1n_16R"
      },
      "source": [
        "### Define Conditional probability (CP) function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zCHwprX_9v7"
      },
      "outputs": [],
      "source": [
        "# Function to calculate conditional probability\n",
        "def calculate_conditional_probability(target_type, given_type, neighbors_map, all_features_gdf):\n",
        "    \"\"\"\n",
        "    Calculates the Conditional Probability P(target_type | given_type).\n",
        "\n",
        "    Args:\n",
        "        target_type (str): The feature type we are checking for (e.g., 'school').\n",
        "        given_type (str): The feature type that must be present (e.g., 'diffusion_tube').\n",
        "        neighbors_map (dict): The neighborhood graph.\n",
        "        all_features_gdf (gpd.GeoDataFrame): GeoDataFrame of all feature instances.\n",
        "\n",
        "    Returns:\n",
        "        float: The calculated conditional probability.\n",
        "    \"\"\"\n",
        "    # 1. Get the total number of instances of the 'given' feature type (the denominator).\n",
        "    total_given_instances = len(all_features_gdf[all_features_gdf['feature_type'] == given_type])\n",
        "    if total_given_instances == 0:\n",
        "        print(f\"No instances of '{given_type}' found.\")\n",
        "        return 0.0\n",
        "\n",
        "    # 2. Count the number of 'given' instances that have at least one 'target' neighbor.\n",
        "    co_located_count = 0\n",
        "    instances_of_given_type = all_features_gdf[all_features_gdf['feature_type'] == given_type]\n",
        "\n",
        "    for _, instance_row in instances_of_given_type.iterrows():\n",
        "        instance_uid = instance_row['uid']\n",
        "        # Use a set comprehension for efficient lookup of neighbor types\n",
        "        neighbors_of_instance = {n_type for _, n_type in neighbors_map.get(instance_uid, [])}\n",
        "\n",
        "        # Check if the target type is in the set of neighbors\n",
        "        if target_type in neighbors_of_instance:\n",
        "            co_located_count += 1\n",
        "\n",
        "    # 3. Calculate the conditional probability.\n",
        "    conditional_prob = co_located_count / total_given_instances\n",
        "    return conditional_prob\n",
        "\n",
        "def calculate_all_conditional_probabilities_for_prevalent_patterns(prevalent_patterns, neighbors_map, all_features_gdf):\n",
        "    \"\"\"\n",
        "    Calculates conditional probabilities for all 2-item prevalent patterns.\n",
        "\n",
        "    Args:\n",
        "        prevalent_patterns (dict): A dictionary of prevalent patterns, e.g.,\n",
        "                                   {frozenset({'A', 'B'}): {'pi': 0.5, 'pr': 0.8}}\n",
        "        neighbors_map (dict): The neighborhood graph.\n",
        "        all_features_gdf (gpd.GeoDataFrame): GeoDataFrame of all feature instances.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary with conditional probabilities for each pair.\n",
        "    \"\"\"\n",
        "    conditional_probs = {}\n",
        "    for pattern_key in prevalent_patterns.keys():\n",
        "        f_type1, f_type2 = list(pattern_key)\n",
        "\n",
        "        # Calculate P(f_type2 | f_type1)\n",
        "        prob_f2_given_f1 = calculate_conditional_probability(f_type2, f_type1, neighbors_map, all_features_gdf)\n",
        "        conditional_probs[f\"{f_type2} | {f_type1}\"] = prob_f2_given_f1\n",
        "\n",
        "        # Calculate P(f_type1 | f_type2)\n",
        "        prob_f1_given_f2 = calculate_conditional_probability(f_type1, f_type2, neighbors_map, all_features_gdf)\n",
        "        conditional_probs[f\"{f_type1} | {f_type2}\"] = prob_f1_given_f2\n",
        "\n",
        "    return conditional_probs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKDxLCQ7IYap"
      },
      "source": [
        "### Calculate Co-location Patterns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PmgQSH_rNIi"
      },
      "outputs": [],
      "source": [
        "# --- Co-location Pattern Mining Loop (including distance threshold) ---\n",
        "print(\"\\n--- Phase 3: Running Co-location Pattern Mining for different thresholds ---\")\n",
        "\n",
        "# Lists to collect all results across ALL distance, PI, and PR combinations\n",
        "all_prevalent_patterns_combined_list = [] # For combined prevalent patterns CSV\n",
        "all_maximal_patterns_combined_list = [] # For combined maximal patterns CSV\n",
        "\n",
        "# Outer loop for distance thresholds\n",
        "for current_distance_threshold_m in distance_threshold_m:\n",
        "    print(f\"\\n===== Running Co-location for Distance: {current_distance_threshold_m}m =====\")\n",
        "\n",
        "    # Recalculate neighbors_map for the current distance threshold\n",
        "    neighbors_map = build_neighborhood_graph(current_distance_threshold_m, all_features_gdf, coords, tree)\n",
        "\n",
        "    # Nested loops to iterate through all PI and PR combinations\n",
        "    for current_min_pr in pr_thresholds: # Outer loop for PR\n",
        "        for current_min_pi in pi_thresholds: # Inner loop for PI\n",
        "            print(f\"\\n--- Current Thresholds (D={current_distance_threshold_m}m): Min PI={current_min_pi:.2f}, Min PR={current_min_pr:.2f} ---\")\n",
        "\n",
        "            # Dictionary to store prevalent patterns for this specific run (PI/PR/Distance)\n",
        "            all_prevalent_patterns_current_run = {\n",
        "                2: {},\n",
        "                3: {},\n",
        "                4: {}\n",
        "            }\n",
        "\n",
        "            # --- Mining 2-item patterns ---\n",
        "            filtered_unique_feature_types = [\n",
        "                f_type for f_type in unique_feature_types\n",
        "                if f_type in all_features_gdf['feature_type'].unique()\n",
        "            ]\n",
        "            candidate_2_item_patterns = list(itertools.combinations(filtered_unique_feature_types, 2))\n",
        "            print(f\"   Searching for 2-item co-location patterns ({len(candidate_2_item_patterns)} candidates)...\")\n",
        "\n",
        "            for F1_type, F2_type in candidate_2_item_patterns:\n",
        "                pattern_key = frozenset([F1_type, F2_type])\n",
        "                pattern_pi, pattern_pr = calculate_pattern_prevalence(\n",
        "                    pattern_key, neighbors_map, all_features_gdf\n",
        "                )\n",
        "\n",
        "                if pattern_pi >= current_min_pi and pattern_pr >= current_min_pr:\n",
        "                    all_prevalent_patterns_current_run[2][pattern_key] = {'pi': pattern_pi, 'pr': pattern_pr}\n",
        "            print(f\"   Total prevalent 2-item patterns found: {len(all_prevalent_patterns_current_run[2])}\")\n",
        "\n",
        "\n",
        "            # --- Mining 3-item patterns (Apriori-like) ---\n",
        "            print(\"\\n   Searching for 3-item co-location patterns...\")\n",
        "            frequent_2_item_keys = all_prevalent_patterns_current_run[2].keys()\n",
        "            frequent_2_item_types = [list(p) for p in frequent_2_item_keys]\n",
        "\n",
        "            candidate_3_item_patterns_set = set()\n",
        "            for i in range(len(frequent_2_item_types)):\n",
        "                for j in range(i + 1, len(frequent_2_item_types)):\n",
        "                    p1 = set(frequent_2_item_types[i])\n",
        "                    p2 = set(frequent_2_item_types[j])\n",
        "                    union_pattern = frozenset(p1.union(p2))\n",
        "                    if len(union_pattern) == 3:\n",
        "                        is_candidate = True\n",
        "                        for subset in itertools.combinations(union_pattern, 2):\n",
        "                            if frozenset(subset) not in frequent_2_item_keys:\n",
        "                                is_candidate = False\n",
        "                                break\n",
        "                        if is_candidate:\n",
        "                            candidate_3_item_patterns_set.add(union_pattern)\n",
        "\n",
        "            candidate_3_item_patterns = [list(p) for p in candidate_3_item_patterns_set]\n",
        "\n",
        "            for pattern_types in candidate_3_item_patterns:\n",
        "                pattern_key = frozenset(pattern_types)\n",
        "                pattern_pi, pattern_pr = calculate_pattern_prevalence(\n",
        "                    pattern_key, neighbors_map, all_features_gdf\n",
        "                )\n",
        "\n",
        "                if pattern_pi >= current_min_pi and pattern_pr >= current_min_pr:\n",
        "                    all_prevalent_patterns_current_run[3][pattern_key] = {'pi': pattern_pi, 'pr': pattern_pr}\n",
        "            print(f\"   Total prevalent 3-item patterns found: {len(all_prevalent_patterns_current_run[3])}\")\n",
        "\n",
        "\n",
        "            # --- Mining 4-item patterns (Apriori-like) ---\n",
        "            print(\"\\n   Searching for 4-item co-location patterns...\")\n",
        "            frequent_3_item_keys = all_prevalent_patterns_current_run[3].keys()\n",
        "            frequent_3_item_types = [list(p) for p in frequent_3_item_keys]\n",
        "\n",
        "            candidate_4_item_patterns_set = set()\n",
        "            for i in range(len(frequent_3_item_types)):\n",
        "                for j in range(i + 1, len(frequent_3_item_types)):\n",
        "                    p1 = set(frequent_3_item_types[i])\n",
        "                    p2 = set(frequent_3_item_types[j])\n",
        "                    union_pattern = frozenset(p1.union(p2))\n",
        "                    if len(union_pattern) == 4:\n",
        "                        is_candidate = True\n",
        "                        for subset in itertools.combinations(union_pattern, 3):\n",
        "                            if frozenset(subset) not in frequent_3_item_keys:\n",
        "                                is_candidate = False\n",
        "                                break\n",
        "                        if is_candidate:\n",
        "                            candidate_4_item_patterns_set.add(union_pattern)\n",
        "\n",
        "            candidate_4_item_patterns = [list(p) for p in candidate_4_item_patterns_set]\n",
        "\n",
        "            for pattern_types in candidate_4_item_patterns:\n",
        "                pattern_key = frozenset(pattern_types)\n",
        "                pattern_pi, pattern_pr = calculate_pattern_prevalence(\n",
        "                    pattern_key, neighbors_map, all_features_gdf\n",
        "                )\n",
        "\n",
        "                if pattern_pi >= current_min_pi and pattern_pr >= current_min_pr:\n",
        "                    all_prevalent_patterns_current_run[4][pattern_key] = {'pi': pattern_pi, 'pr': pattern_pr}\n",
        "            print(f\"   Total prevalent 4-item patterns found: {len(all_prevalent_patterns_current_run[4])}\")\n",
        "\n",
        "            # --- Summary and Saving for the Current Threshold Combination (PI/PR/Distance) ---\n",
        "            print(f\"\\n   --- Summary for D={current_distance_threshold_m}m, Min PI={current_min_pi:.2f}, Min PR={current_min_pr:.2f} ---\")\n",
        "            summary_data_prevalent_current_run = [] # Renamed for clarity\n",
        "\n",
        "            found_any_patterns_current_run = False\n",
        "\n",
        "            for k in sorted(all_prevalent_patterns_current_run.keys()):\n",
        "                if all_prevalent_patterns_current_run[k]:\n",
        "                    found_any_patterns_current_run = True\n",
        "                    print(f\"\\n   Prevalent {k}-item Patterns:\")\n",
        "                    sorted_patterns = sorted(all_prevalent_patterns_current_run[k].items(), key=lambda item: item[1]['pi'], reverse=True)\n",
        "                    for pattern, metrics in sorted_patterns:\n",
        "                        pattern_set = set(pattern)\n",
        "                        print(f\"    Pattern: {pattern_set}, PI: {metrics['pi']:.4f}, PR: {metrics['pr']:.4f}\")\n",
        "\n",
        "                        summary_data_prevalent_current_run.append({\n",
        "                            'distance_threshold_m': current_distance_threshold_m,\n",
        "                            'min_pi_threshold': current_min_pi,\n",
        "                            'min_pr_threshold': current_min_pr,\n",
        "                            'pattern_length': k,\n",
        "                            'pattern': \", \".join(sorted(pattern_set)),\n",
        "                            'participation_index': metrics['pi'],\n",
        "                            'participation_ratio': metrics['pr']\n",
        "                        })\n",
        "\n",
        "            if not found_any_patterns_current_run:\n",
        "                print(\"   No prevalent co-location patterns found for this combination of thresholds.\")\n",
        "\n",
        "            # Save individual prevalent patterns CSV\n",
        "            if summary_data_prevalent_current_run:\n",
        "                patterns_summary_df_current_run = pd.DataFrame(summary_data_prevalent_current_run)\n",
        "                patterns_summary_df_current_run = patterns_summary_df_current_run.sort_values(\n",
        "                    by=['distance_threshold_m', 'pattern_length', 'participation_index'],\n",
        "                    ascending=[True, True, False]\n",
        "                ).reset_index(drop=True)\n",
        "\n",
        "                formatted_dist_str = f\"{current_distance_threshold_m}m\"\n",
        "                formatted_pi_str = f\"{current_min_pi:.2f}\".replace('.', '')\n",
        "                formatted_pr_str = f\"{current_min_pr:.2f}\".replace('.', '')\n",
        "                output_filename = f\"patterns_summary_D{formatted_dist_str}_PI{formatted_pi_str}_PR{formatted_pr_str}.csv\"\n",
        "                output_filepath = os.path.join(output_dir_colocation, output_filename)\n",
        "\n",
        "                patterns_summary_df_current_run.to_csv(output_filepath, index=False)\n",
        "                print(f\"   Prevalent co-location pattern summary saved to: {output_filepath}\")\n",
        "            else:\n",
        "                print(\"   No prevalent patterns CSV file saved for this run as no patterns were found.\")\n",
        "\n",
        "            # Extend the list for the final combined CSV for ALL prevalent patterns\n",
        "            all_prevalent_patterns_combined_list.extend(summary_data_prevalent_current_run)\n",
        "\n",
        "\n",
        "            # --- Identify Maximal Co-location Patterns for the Current Thresholds ---\n",
        "            print(f\"\\n   --- Identifying Maximal Co-location Patterns for D={current_distance_threshold_m}m, PI={current_min_pi:.2f}, PR={current_min_pr:.2f} ---\")\n",
        "\n",
        "            # Flatten all prevalent patterns for the CURRENT RUN into a list: (pattern, metrics)\n",
        "            # This is crucial for maximal pattern finding - only consider patterns from the same run\n",
        "            all_patterns_for_maximal_check = []\n",
        "            for k_len in sorted(all_prevalent_patterns_current_run.keys(), reverse=True):\n",
        "                for pattern, metrics in all_prevalent_patterns_current_run[k_len].items():\n",
        "                    all_patterns_for_maximal_check.append((pattern, metrics))\n",
        "\n",
        "            maximal_patterns = []\n",
        "            summary_data_maximal_current_run = [] # Collects data for current run's maximal CSV\n",
        "\n",
        "            if not all_patterns_for_maximal_check:\n",
        "                print(\"   No prevalent patterns found, thus no maximal patterns for this run.\")\n",
        "            else:\n",
        "                for i, (pattern_i, metrics_i) in enumerate(all_patterns_for_maximal_check):\n",
        "                    is_maximal = True\n",
        "                    for j, (pattern_j, _) in enumerate(all_patterns_for_maximal_check):\n",
        "                        if i != j:\n",
        "                            # Check if pattern_i is a subset of pattern_j AND pattern_j is larger\n",
        "                            if pattern_i.issubset(pattern_j) and len(pattern_i) < len(pattern_j):\n",
        "                                is_maximal = False\n",
        "                                break # Not maximal, found a larger super-pattern\n",
        "                    if is_maximal:\n",
        "                        maximal_patterns.append((pattern_i, metrics_i))\n",
        "                        summary_data_maximal_current_run.append({\n",
        "                            'distance_threshold_m': current_distance_threshold_m, # ADDED: Distance threshold\n",
        "                            'min_pi_threshold': current_min_pi,\n",
        "                            'min_pr_threshold': current_min_pr,\n",
        "                            'pattern_length': len(pattern_i),\n",
        "                            'pattern': \", \".join(sorted(pattern_i)),\n",
        "                            'participation_index': metrics_i['pi'],\n",
        "                            'participation_ratio': metrics_i['pr']\n",
        "                        })\n",
        "\n",
        "                print(f\"   Total maximal co-location patterns found: {len(maximal_patterns)}\")\n",
        "\n",
        "                if maximal_patterns:\n",
        "                    print(\"\\n   Maximal Co-location Patterns:\")\n",
        "                    for pattern, metrics in sorted(maximal_patterns, key=lambda x: x[1]['pi'], reverse=True):\n",
        "                        print(f\"    Pattern: {set(pattern)}, PI: {metrics['pi']:.4f}, PR: {metrics['pr']:.4f}\")\n",
        "\n",
        "                    # Save individual maximal patterns CSV\n",
        "                    df_maximal_current_run = pd.DataFrame(summary_data_maximal_current_run).sort_values(\n",
        "                        by=['distance_threshold_m', 'pattern_length', 'participation_index'], # ADDED: Sort by distance\n",
        "                        ascending=[True, True, False]).reset_index(drop=True)\n",
        "\n",
        "                    formatted_dist_str = f\"{current_distance_threshold_m}m\" # NEW: Format distance\n",
        "                    pi_str = f\"{current_min_pi:.2f}\".replace('.', '')\n",
        "                    pr_str = f\"{current_min_pr:.2f}\".replace('.', '')\n",
        "                    filename = f\"maximal_patterns_D{formatted_dist_str}_PI{pi_str}_PR{pr_str}.csv\" # UPDATED FILENAME\n",
        "                    filepath = os.path.join(output_dir_colocation, filename)\n",
        "\n",
        "                    df_maximal_current_run.to_csv(filepath, index=False)\n",
        "                    print(f\"   Maximal co-location pattern summary saved to: {filepath}\")\n",
        "                else:\n",
        "                    print(\"   No maximal co-location patterns found for this combination of thresholds.\")\n",
        "\n",
        "            # Extend the list for the final combined CSV for ALL maximal patterns\n",
        "            all_maximal_patterns_combined_list.extend(summary_data_maximal_current_run)\n",
        "\n",
        "\n",
        "# --- Final Combined CSV Files ---\n",
        "print(\"\\n--- Phase 4: Generating combined CSV files for all prevalent and maximal patterns ---\")\n",
        "\n",
        "# Combine all prevalent patterns into one DataFrame\n",
        "if all_prevalent_patterns_combined_list: # Renamed from all_results_summary_df_list\n",
        "    final_prevalent_patterns_df = pd.DataFrame(all_prevalent_patterns_combined_list)\n",
        "    final_prevalent_patterns_df = final_prevalent_patterns_df.sort_values(\n",
        "        by=['distance_threshold_m', 'min_pi_threshold', 'min_pr_threshold', 'pattern_length', 'participation_index'], # Added distance for sorting\n",
        "        ascending=[True, True, True, True, False]\n",
        "    ).reset_index(drop=True)\n",
        "\n",
        "    combined_prevalent_output_filepath = os.path.join(output_dir_colocation, \"all_prevalent_patterns_combined.csv\")\n",
        "    final_prevalent_patterns_df.to_csv(combined_prevalent_output_filepath, index=False)\n",
        "    print(f\"All prevalent patterns combined and saved to: {combined_prevalent_output_filepath}\")\n",
        "else:\n",
        "    print(\"No prevalent patterns found across all runs to combine.\")\n",
        "\n",
        "# Combine all maximal patterns into one DataFrame\n",
        "if all_maximal_patterns_combined_list: # Renamed from all_maximal_patterns_df_list\n",
        "    final_maximal_patterns_df = pd.DataFrame(all_maximal_patterns_combined_list)\n",
        "    final_maximal_patterns_df = final_maximal_patterns_df.sort_values(\n",
        "        by=['distance_threshold_m', 'min_pi_threshold', 'min_pr_threshold', 'pattern_length', 'participation_index'], # Added distance for sorting\n",
        "        ascending=[True, True, True, True, False]\n",
        "    ).reset_index(drop=True)\n",
        "\n",
        "    combined_maximal_output_filepath = os.path.join(output_dir_colocation, \"all_maximal_patterns_combined.csv\")\n",
        "    final_maximal_patterns_df.to_csv(combined_maximal_output_filepath, index=False)\n",
        "    print(f\"All maximal patterns combined and saved to: {combined_maximal_output_filepath}\")\n",
        "else:\n",
        "    print(\"No maximal patterns found across all runs to combine.\")\n",
        "\n",
        "print(\"\\n--- Co-location Pattern Mining Process Completed ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z79_gpYsqm3s"
      },
      "source": [
        "### Calculate conditional probability (cp) of size-2 patterns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bKklR6GjFH7"
      },
      "outputs": [],
      "source": [
        "# --- Phase 5: Calculating and Saving Conditional Probabilities ---\n",
        "\n",
        "def calculate_and_save_conditional_probabilities(combined_prevalent_filepath, output_dir, neighbors_map, all_features_gdf):\n",
        "    \"\"\"\n",
        "    Reads the combined prevalent patterns file, calculates conditional probabilities for 2-item patterns,\n",
        "    and saves the results to a new CSV file.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Phase 5: Calculating Conditional Probabilities for all 2-item prevalent patterns ---\")\n",
        "\n",
        "    if not os.path.exists(combined_prevalent_filepath):\n",
        "        print(f\"File not found: {combined_prevalent_filepath}. Skipping conditional probability calculation.\")\n",
        "        return\n",
        "\n",
        "    # Read the combined prevalent patterns CSV\n",
        "    prevalent_df = pd.read_csv(combined_prevalent_filepath)\n",
        "    # Filter for only 2-item patterns\n",
        "    two_item_patterns_df = prevalent_df[prevalent_df['pattern_length'] == 2].copy()\n",
        "\n",
        "    if two_item_patterns_df.empty:\n",
        "        print(\"No 2-item prevalent patterns found in the combined file to calculate conditional probabilities for.\")\n",
        "        return\n",
        "\n",
        "    # Create a new list to store the results\n",
        "    conditional_prob_data = []\n",
        "\n",
        "    # Iterate through each unique 2-item pattern to calculate probabilities\n",
        "    unique_patterns = two_item_patterns_df.drop_duplicates(subset=['distance_threshold_m', 'pattern'])\n",
        "\n",
        "    for _, row in unique_patterns.iterrows():\n",
        "        pattern_str = row['pattern']\n",
        "        # The pattern string is like \"A, B\", split and strip whitespace\n",
        "        f_type1, f_type2 = [s.strip() for s in pattern_str.split(',')]\n",
        "\n",
        "        # Re-build or access the correct map for the current distance threshold if not already available\n",
        "        current_distance_threshold_m = row['distance_threshold_m']\n",
        "        current_neighbors_map = build_neighborhood_graph(current_distance_threshold_m, all_features_gdf, coords, tree)\n",
        "\n",
        "        # Calculate P(f_type2 | f_type1)\n",
        "        prob_f2_given_f1 = calculate_conditional_probability(f_type2, f_type1, current_neighbors_map, all_features_gdf)\n",
        "\n",
        "        # Calculate P(f_type1 | f_type2)\n",
        "        prob_f1_given_f2 = calculate_conditional_probability(f_type1, f_type2, current_neighbors_map, all_features_gdf)\n",
        "\n",
        "        conditional_prob_data.append({\n",
        "            'distance_threshold_m': current_distance_threshold_m,\n",
        "            'pattern': pattern_str,\n",
        "            'P(f_type2 | f_type1)': f\"{f_type2} | {f_type1}\",\n",
        "            'conditional_probability_value': prob_f2_given_f1\n",
        "        })\n",
        "        conditional_prob_data.append({\n",
        "            'distance_threshold_m': current_distance_threshold_m,\n",
        "            'pattern': pattern_str,\n",
        "            'P(f_type1 | f_type2)': f\"{f_type1} | {f_type2}\",\n",
        "            'conditional_probability_value': prob_f1_given_f2\n",
        "        })\n",
        "\n",
        "    # Create a DataFrame and save to CSV\n",
        "    if conditional_prob_data:\n",
        "        conditional_probs_df = pd.DataFrame(conditional_prob_data)\n",
        "        conditional_probs_df.to_csv(os.path.join(output_dir, \"conditional_probabilities_combined.csv\"), index=False)\n",
        "        print(f\"Conditional probabilities for all 2-item patterns saved to: {os.path.join(output_dir, 'conditional_probabilities_combined.csv')}\")\n",
        "    else:\n",
        "        print(\"No conditional probabilities were calculated.\")\n",
        "\n",
        "# --- Execute the new function after the main loop finishes ---\n",
        "if all_prevalent_patterns_combined_list:\n",
        "    calculate_and_save_conditional_probabilities(\n",
        "        combined_prevalent_output_filepath,\n",
        "        output_dir_colocation,\n",
        "        neighbors_map, # This will be the last map generated by the loop, but it will be re-built inside the function\n",
        "        all_features_gdf\n",
        "    )\n",
        "else:\n",
        "    print(\"Skipping conditional probability calculation because no prevalent patterns were found.\")\n",
        "\n",
        "print(\"\\n--- All Processes Completed ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqs3v5EDjpdW"
      },
      "outputs": [],
      "source": [
        "cp_read = pd.read_csv(\"/content/colocation_data/conditional_probabilities_combined.csv\")\n",
        "display(cp_read)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNItHHjR7QKL"
      },
      "outputs": [],
      "source": [
        "# --- Final Combined CSV Files ---\n",
        "print(\"\\n--- Phase 4: Generating combined CSV files for all prevalent and maximal patterns ---\")\n",
        "\n",
        "# Combine all prevalent patterns into one DataFrame\n",
        "if all_prevalent_patterns_combined_list:\n",
        "    final_prevalent_patterns_df = pd.DataFrame(all_prevalent_patterns_combined_list)\n",
        "    final_prevalent_patterns_df = final_prevalent_patterns_df.sort_values(\n",
        "        by=['distance_threshold_m', 'min_pi_threshold', 'min_pr_threshold', 'pattern_length', 'participation_index'],\n",
        "        ascending=[True, True, True, True, False]\n",
        "    ).reset_index(drop=True)\n",
        "\n",
        "    combined_prevalent_output_filepath = os.path.join(output_dir_colocation, \"all_prevalent_patterns_combined.csv\")\n",
        "    final_prevalent_patterns_df.to_csv(combined_prevalent_output_filepath, index=False)\n",
        "    print(f\"All prevalent patterns combined and saved to: {combined_prevalent_output_filepath}\")\n",
        "    print(\"\\nFirst 10 rows of the combined prevalent patterns summary:\")\n",
        "    print(final_prevalent_patterns_df.head(10)) # Added head print for prevalent\n",
        "else:\n",
        "    print(\"No prevalent patterns found across all runs to combine.\")\n",
        "\n",
        "# Combine all maximal patterns into one DataFrame\n",
        "if all_maximal_patterns_combined_list:\n",
        "    final_maximal_patterns_df = pd.DataFrame(all_maximal_patterns_combined_list)\n",
        "    final_maximal_patterns_df = final_maximal_patterns_df.sort_values(\n",
        "        by=['distance_threshold_m', 'min_pi_threshold', 'min_pr_threshold', 'pattern_length', 'participation_index'],\n",
        "        ascending=[True, True, True, True, False]\n",
        "    ).reset_index(drop=True)\n",
        "\n",
        "    combined_maximal_output_filepath = os.path.join(output_dir_colocation, \"all_maximal_patterns_combined.csv\")\n",
        "    final_maximal_patterns_df.to_csv(combined_maximal_output_filepath, index=False)\n",
        "    print(f\"All maximal patterns combined and saved to: {combined_maximal_output_filepath}\")\n",
        "    print(\"\\nFirst 10 rows of the combined maximal patterns summary:\")\n",
        "    print(final_maximal_patterns_df.head(10)) # Added head print for maximal\n",
        "else:\n",
        "    print(\"No maximal patterns found across all runs to combine.\")\n",
        "\n",
        "print(\"\\n--- Co-location Pattern Mining Process Completed ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7MoBPucgy0HA"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "folder_to_zip = \"colocation_data\"\n",
        "zip_filename = \"colocation_results_archive\"\n",
        "\n",
        "# Define the output path for the zip file.\n",
        "output_zip_path = os.path.join(os.getcwd(), zip_filename)\n",
        "\n",
        "print(f\"Attempting to zip folder: {folder_to_zip}\")\n",
        "\n",
        "try:\n",
        "    shutil.make_archive(output_zip_path, 'zip', folder_to_zip)\n",
        "    print(f\"Successfully created zip file: {output_filename}.zip\")\n",
        "    print(f\"The zip file is located at: {output_zip_path}.zip\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The folder '{folder_to_zip}' was not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during zipping: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEc5k16iYAKd"
      },
      "source": [
        "### Visualisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfLT8tzJ-j_Y"
      },
      "source": [
        "compare most and least frequent patterns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6WLQWF4XEW1"
      },
      "outputs": [],
      "source": [
        "# --- Configuration for Visualization ---\n",
        "output_dir_colocation = \"colocation_data\"\n",
        "\n",
        "# Define the specific thresholds for the comparison\n",
        "D_COMPARE = 600  # meters\n",
        "MIN_PI_COMPARE = 0.5\n",
        "\n",
        "# Define pattern size for comparison (e.g., compare only size-2 patterns)\n",
        "PATTERN_SIZE_COMPARE = 2\n",
        "\n",
        "# --- Load the combined prevalent patterns data ---\n",
        "combined_prevalent_output_filepath = os.path.join(output_dir_colocation, \"all_prevalent_patterns_combined.csv\")\n",
        "\n",
        "try:\n",
        "    df_all_prevalent = pd.read_csv(combined_prevalent_output_filepath)\n",
        "    print(f\"Loaded combined prevalent patterns from: {combined_prevalent_output_filepath}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Combined prevalent patterns file not found at {combined_prevalent_output_filepath}\")\n",
        "    exit() # Exit if the file isn't found\n",
        "\n",
        "# --- Filter for the specific comparison scenario ---\n",
        "df_scenario = df_all_prevalent[\n",
        "    (df_all_prevalent['distance_threshold_m'] == D_COMPARE) &\n",
        "    (df_all_prevalent['min_pi_threshold'] == MIN_PI_COMPARE) &\n",
        "    (df_all_prevalent['pattern_length'] == PATTERN_SIZE_COMPARE) # Filter by pattern size\n",
        "].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
        "\n",
        "if df_scenario.empty:\n",
        "    print(f\"\\nNo prevalent patterns found for D={D_COMPARE}m, Min PI={MIN_PI_COMPARE}, Size={PATTERN_SIZE_COMPARE}.\")\n",
        "else:\n",
        "    # --- Identify Most and Least Frequent Patterns ---\n",
        "    df_scenario_sorted = df_scenario.sort_values(by='participation_index', ascending=False)\n",
        "\n",
        "    most_frequent_pattern = df_scenario_sorted.iloc[0]\n",
        "    # Ensure there's more than one pattern to pick a 'least' from\n",
        "    if len(df_scenario_sorted) > 1:\n",
        "        least_frequent_pattern = df_scenario_sorted.iloc[-1]\n",
        "    else:\n",
        "        least_frequent_pattern = None # Only one pattern, cannot compare least\n",
        "\n",
        "    print(f\"\\n--- Comparison for D={D_COMPARE}m, Min PI={MIN_PI_COMPARE}, Size={PATTERN_SIZE_COMPARE} ---\")\n",
        "\n",
        "    if most_frequent_pattern is not None:\n",
        "        print(\"\\nMost Frequent Pattern:\")\n",
        "        print(f\"  Pattern: {{{most_frequent_pattern['pattern']}}}, PI: {most_frequent_pattern['participation_index']:.4f}, PR: {most_frequent_pattern['participation_ratio']:.4f}\")\n",
        "\n",
        "    if least_frequent_pattern is not None:\n",
        "        print(\"\\nLeast Frequent Pattern:\")\n",
        "        print(f\"  Pattern: {{{least_frequent_pattern['pattern']}}}, PI: {least_frequent_pattern['participation_index']:.4f}, PR: {least_frequent_pattern['participation_ratio']:.4f}\")\n",
        "    elif len(df_scenario_sorted) == 1:\n",
        "        print(\"Only one pattern found for this scenario, cannot compare most vs. least.\")\n",
        "    else:\n",
        "        print(\"No patterns found to compare.\")\n",
        "\n",
        "    # --- Visualization (Bar Chart) ---\n",
        "    if most_frequent_pattern is not None and least_frequent_pattern is not None:\n",
        "        patterns_to_plot = pd.DataFrame([most_frequent_pattern, least_frequent_pattern])\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.barplot(x='pattern', y='participation_index', data=patterns_to_plot, palette='viridis')\n",
        "        plt.title(f'Most vs. Least Frequent Size-{PATTERN_SIZE_COMPARE} Co-location Patterns\\n'\n",
        "                  f'(D={D_COMPARE}m, Min PI={MIN_PI_COMPARE})')\n",
        "        plt.xlabel('Co-location Pattern')\n",
        "        plt.ylabel('Participation Index (PI)')\n",
        "        plt.ylim(0, 1) # PI is between 0 and 1\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save the plot\n",
        "        plot_filename = f\"most_least_frequent_D{D_COMPARE}min_PI{str(MIN_PI_COMPARE).replace('.', '')}_Size{PATTERN_SIZE_COMPARE}.png\"\n",
        "        plot_filepath = os.path.join(output_dir_colocation, plot_filename)\n",
        "        plt.savefig(plot_filepath)\n",
        "        print(f\"\\nVisualization saved to: {plot_filepath}\")\n",
        "        plt.show()\n",
        "\n",
        "    elif most_frequent_pattern is not None:\n",
        "        print(\"Cannot create comparison plot: Only one prevalent pattern found for this scenario.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jr1-2jDM-sk2"
      },
      "source": [
        "compare most and least frequent patterns (diffusion tube)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KNP4c3WaWhR"
      },
      "outputs": [],
      "source": [
        "# --- Configuration for Visualization ---\n",
        "output_dir_colocation = \"colocation_data\" # Ensure this matches your output folder\n",
        "\n",
        "# Define the specific thresholds for the comparison\n",
        "D_COMPARE = 600  # meters\n",
        "MIN_PI_COMPARE = 0.01\n",
        "TARGET_FEATURE_TYPE = 'diffusion_tube'\n",
        "PATTERN_SIZE_COMPARE = 2 # choose: 2, 3, 4, or None\n",
        "\n",
        "# --- Load the combined prevalent patterns data ---\n",
        "combined_prevalent_output_filepath = os.path.join(output_dir_colocation, \"all_prevalent_patterns_combined.csv\")\n",
        "\n",
        "try:\n",
        "    df_all_prevalent = pd.read_csv(combined_prevalent_output_filepath)\n",
        "    print(f\"Loaded combined prevalent patterns from: {combined_prevalent_output_filepath}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Combined prevalent patterns file not found at {combined_prevalent_output_filepath}\")\n",
        "    exit() # Exit if the file isn't found\n",
        "\n",
        "# --- Filter for the specific comparison scenario (distance, PI, PR) ---\n",
        "df_scenario = df_all_prevalent[(df_all_prevalent['distance_threshold_m'] == D_COMPARE) &\n",
        " (df_all_prevalent['min_pi_threshold'] == MIN_PI_COMPARE)].copy()\n",
        "\n",
        "# --- NEW FILTER: Filter for patterns containing the target feature type ---\n",
        "# The 'pattern' column is stored as a comma-separated string, so we check for substring.\n",
        "df_target_feature_patterns = df_scenario[df_scenario['pattern'].str.contains(TARGET_FEATURE_TYPE, na=False)].copy()\n",
        "\n",
        "# Optional: Further filter by pattern size if PATTERN_SIZE_COMPARE is set\n",
        "if PATTERN_SIZE_COMPARE is not None:\n",
        "    df_target_feature_patterns = df_target_feature_patterns[\n",
        "        df_target_feature_patterns['pattern_length'] == PATTERN_SIZE_COMPARE\n",
        "    ].copy()\n",
        "\n",
        "if df_target_feature_patterns.empty:\n",
        "    print(f\"\\nNo prevalent patterns containing '{TARGET_FEATURE_TYPE}' found for D={D_COMPARE}m, Min PI={MIN_PI_COMPARE}.\")\n",
        "    if PATTERN_SIZE_COMPARE is not None:\n",
        "        print(f\"Consider adjusting PATTERN_SIZE_COMPARE or setting it to None.\")\n",
        "else:\n",
        "    # --- Identify Most and Least Frequent Patterns among the filtered set ---\n",
        "    df_filtered_sorted = df_target_feature_patterns.sort_values(by='participation_index', ascending=False)\n",
        "\n",
        "    most_frequent_pattern = df_filtered_sorted.iloc[0]\n",
        "    if len(df_filtered_sorted) > 1:\n",
        "        least_frequent_pattern = df_filtered_sorted.iloc[-1]\n",
        "    else:\n",
        "        least_frequent_pattern = None # Only one pattern, cannot compare least\n",
        "\n",
        "    print(f\"\\n--- Comparison for patterns containing '{TARGET_FEATURE_TYPE}' at D={D_COMPARE}m, Min PI={MIN_PI_COMPARE} ---\")\n",
        "    if PATTERN_SIZE_COMPARE is not None:\n",
        "        print(f\"--- (Only considering size-{PATTERN_SIZE_COMPARE} patterns) ---\")\n",
        "\n",
        "\n",
        "    if most_frequent_pattern is not None:\n",
        "        print(\"\\nMost Frequent Pattern:\")\n",
        "        print(f\"  Pattern: {{{most_frequent_pattern['pattern']}}}, PI: {most_frequent_pattern['participation_index']:.4f}, PR: {most_frequent_pattern['participation_ratio']:.4f}\")\n",
        "\n",
        "    if least_frequent_pattern is not None:\n",
        "        print(\"\\nLeast Frequent Pattern:\")\n",
        "        print(f\"  Pattern: {{{least_frequent_pattern['pattern']}}}, PI: {least_frequent_pattern['participation_index']:.4f}, PR: {least_frequent_pattern['participation_ratio']:.4f}\")\n",
        "    elif len(df_filtered_sorted) == 1:\n",
        "        print(f\"Only one pattern containing '{TARGET_FEATURE_TYPE}' found for this scenario, cannot compare most vs. least.\")\n",
        "    else:\n",
        "        print(f\"No patterns containing '{TARGET_FEATURE_TYPE}' found to compare.\")\n",
        "\n",
        "    # --- Visualization (Bar Chart) ---\n",
        "    if most_frequent_pattern is not None and least_frequent_pattern is not None:\n",
        "        patterns_to_plot = pd.DataFrame([most_frequent_pattern, least_frequent_pattern])\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.barplot(x='pattern', y='participation_index', data=patterns_to_plot, palette='viridis')\n",
        "        plt.title(f'Most vs. Least Frequent Co-location Patterns with \"{TARGET_FEATURE_TYPE}\"\\n'\n",
        "                  f'(D={D_COMPARE}m, Min PI={MIN_PI_COMPARE})' +\n",
        "                  (f' - Size {PATTERN_SIZE_COMPARE} Patterns' if PATTERN_SIZE_COMPARE else ''))\n",
        "        plt.xlabel('Co-location Pattern')\n",
        "        plt.ylabel('Participation Index (PI)')\n",
        "        plt.ylim(0, 1) # PI is between 0 and 1\n",
        "        plt.xticks(rotation=0, ha='center')\n",
        "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save the plot\n",
        "        plot_filename = f\"most_least_{TARGET_FEATURE_TYPE}_D{D_COMPARE}m_PI{str(MIN_PI_COMPARE).replace('.', '')}\"\n",
        "        if PATTERN_SIZE_COMPARE is not None:\n",
        "            plot_filename += f\"_Size{PATTERN_SIZE_COMPARE}\"\n",
        "        plot_filename += \".png\"\n",
        "\n",
        "        plot_filepath = os.path.join(output_dir_colocation, plot_filename)\n",
        "        plt.savefig(plot_filepath)\n",
        "        print(f\"\\nVisualization saved to: {plot_filepath}\")\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"Cannot create comparison plot as there are not enough patterns for comparison.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2VsUeTa6hPv"
      },
      "outputs": [],
      "source": [
        "# Reproject to a local projected CRS (e.g., British National Grid) for accurate distance calculation\n",
        "local_crs = \"EPSG:27700\"\n",
        "all_features_gdf = all_features_gdf.to_crs(local_crs)\n",
        "\n",
        "# --- Define the Most Prevalent Pattern and its Colors ---\n",
        "MOST_PREVALENT_PATTERN = ['diffusion_tube', 'kindergarten', 'park', 'school']\n",
        "FEATURE_COLORS = {\n",
        "    'diffusion_tube': 'blue',\n",
        "    'kindergarten': 'purple',\n",
        "    'park': 'green',\n",
        "    'school': 'red'\n",
        "}\n",
        "FEATURE_RADIUS = 2\n",
        "\n",
        "# --- 2. Load and Reproject Boundaries for the Map ---\n",
        "print(\"\\nLoading and reprojecting boundaries...\")\n",
        "sheffield_city_boundary_wgs84 = gpd.read_file(\"City_Boundary.geojson\").to_crs(\"EPSG:4326\")\n",
        "sheffield_ward_boundary_wgs84 = gpd.read_file(\"Sheffield_Wards.geojson\").to_crs(\"EPSG:4326\")\n",
        "\n",
        "# Reproject all features for Folium plotting (back to WGS84)\n",
        "all_features_gdf_wgs84 = all_features_gdf.to_crs(\"EPSG:4326\")\n",
        "\n",
        "# --- 3. Create a Folium Map Object and Add Boundaries ---\n",
        "sheffield_center = [53.3810, -1.4701]\n",
        "m_prevalent_pattern = folium.Map(location=sheffield_center, zoom_start=12)\n",
        "\n",
        "folium.GeoJson(sheffield_city_boundary_wgs84, name=\"Sheffield City Boundary\", style_function=lambda x: {'color': 'black', 'weight': 2, 'fillColor': 'none'}).add_to(m_prevalent_pattern)\n",
        "folium.GeoJson(sheffield_ward_boundary_wgs84, name=\"Sheffield Ward Boundary\", style_function=lambda x: {'color': 'gray', 'weight': 1, 'fillColor': 'none'}).add_to(m_prevalent_pattern)\n",
        "\n",
        "# --- 4. Plot Each Feature in the Prevalent Pattern with Distinct Colors ---\n",
        "print(f\"Plotting the most prevalent pattern: {MOST_PREVALENT_PATTERN}\")\n",
        "for feature_type in MOST_PREVALENT_PATTERN:\n",
        "    # Filter the GeoDataFrame for the current feature type\n",
        "    gdf_filtered = all_features_gdf_wgs84[all_features_gdf_wgs84['feature_type'] == feature_type]\n",
        "\n",
        "    if not gdf_filtered.empty:\n",
        "        # Create a FeatureGroup for each type to enable layer control\n",
        "        feature_group = folium.FeatureGroup(name=f\"{feature_type.title()} Locations\").add_to(m_prevalent_pattern)\n",
        "\n",
        "        # Plot each point as a circle marker\n",
        "        for _, row in gdf_filtered.iterrows():\n",
        "            folium.CircleMarker(\n",
        "                location=[row.geometry.y, row.geometry.x],\n",
        "                radius=FEATURE_RADIUS,\n",
        "                color=FEATURE_COLORS[feature_type],\n",
        "                fill=True,\n",
        "                fill_color=FEATURE_COLORS[feature_type],\n",
        "                fill_opacity=0.8,\n",
        "                popup=f\"Type: {feature_type.title()}\"\n",
        "            ).add_to(feature_group)\n",
        "\n",
        "# Add a Layer Control to toggle layers on/off\n",
        "folium.LayerControl().add_to(m_prevalent_pattern)\n",
        "\n",
        "# Save the map as an HTML file\n",
        "map_filename = \"most_prevalent_pattern_map.html\"\n",
        "m_prevalent_pattern.save(map_filename)\n",
        "print(f\"\\nInteractive map saved to: {map_filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ffN2cgteItR"
      },
      "outputs": [],
      "source": [
        "m_prevalent_pattern"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnvsCdVoR_k4"
      },
      "source": [
        "## Non-prevalent patterns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcJxQH19APEk"
      },
      "source": [
        "### calculate non-prevalent patterns for specific distance and min-prev thresholds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3v8mFuhcQ0n9"
      },
      "outputs": [],
      "source": [
        "# --- Finding Potential Non-prevalent Patterns ---\n",
        "print(\"\\n--- Identifying Potential Non-prevalent Patterns (2-item with very low PI) ---\")\n",
        "\n",
        "# Define the distance threshold for evaluating \"non-prevalent\" proximity.\n",
        "D_FOR_NEGATIVE_CHECK = 600 # meters\n",
        "LOW_PI_THRESHOLD = 0.01 # Define what \"very low PI\" means (e.g., < 1%)\n",
        "\n",
        "# Build the neighborhood graph for this specific distance\n",
        "neighbors_map_for_nonprev_check = build_neighborhood_graph(D_FOR_NEGATIVE_CHECK, all_features_gdf, coords, tree)\n",
        "\n",
        "# Get all unique feature types present in your data\n",
        "actual_unique_feature_types = all_features_gdf['feature_type'].unique().tolist()\n",
        "\n",
        "# Generate all possible 2-item candidate patterns\n",
        "all_2_item_candidates = list(itertools.combinations(actual_unique_feature_types, 2))\n",
        "print(f\"Checking {len(all_2_item_candidates)} total 2-item candidate patterns for low PI at D={D_FOR_NEGATIVE_CHECK}m.\")\n",
        "\n",
        "nonprev_patterns_candidates = []\n",
        "\n",
        "for F1_type, F2_type in all_2_item_candidates:\n",
        "    pattern_key = frozenset([F1_type, F2_type])\n",
        "    pattern_pi, pattern_pr = calculate_pattern_prevalence(\n",
        "        pattern_key, neighbors_map_for_nonprev_check, all_features_gdf\n",
        "    )\n",
        "\n",
        "    # Condition to identify \"non-prevalent\" (very low PI) patterns\n",
        "    # Includes patterns with PI strictly less than LOW_PI_THRESHOLD OR PI exactly 0\n",
        "    if pattern_pi < LOW_PI_THRESHOLD:\n",
        "        nonprev_patterns_candidates.append({\n",
        "            'distance_threshold_m': D_FOR_NEGATIVE_CHECK,\n",
        "            'pattern': \", \".join(sorted(pattern_key)),\n",
        "            'participation_index': pattern_pi,\n",
        "            'participation_ratio': pattern_pr\n",
        "        })\n",
        "\n",
        "\n",
        "if nonprev_patterns_candidates:\n",
        "    df_nonprev_patterns = pd.DataFrame(nonprev_patterns_candidates)\n",
        "    df_nonprev_patterns = df_nonprev_patterns.sort_values(\n",
        "        by='participation_index', ascending=True\n",
        "    ).reset_index(drop=True)\n",
        "\n",
        "    output_filename = f\"potential_non-prevalent_2_item_patterns_D{D_FOR_NEGATIVE_CHECK}m_PI_lt_{str(LOW_PI_THRESHOLD).replace('.', '')}.csv\"\n",
        "    output_filepath = os.path.join(output_dir_colocation, output_filename)\n",
        "    df_nonprev_patterns.to_csv(output_filepath, index=False)\n",
        "\n",
        "    print(f\"\\nFound {len(nonprev_patterns_candidates)} potential non-prevalent 2-item patterns (PI < {LOW_PI_THRESHOLD}).\")\n",
        "    print(f\"Results saved to: {output_filepath}\")\n",
        "    print(\"\\nAll patterns with lowest PI (most 'non-prevalent patterns'):\")\n",
        "    # --- UPDATED LINE TO PRINT ALL RESULTS ---\n",
        "    print(df_nonprev_patterns.to_string()) # Use .to_string() to print the entire DataFrame without truncation\n",
        "else:\n",
        "    print(f\"No 2-item patterns found with PI less than {LOW_PI_THRESHOLD} at {D_FOR_NEGATIVE_CHECK}m.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1d4Gy5UADBa"
      },
      "source": [
        "### calculate non-prevalent patterns from all distance and min_prev thresholds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkpWBUMrUQA-"
      },
      "outputs": [],
      "source": [
        "# --- Phase 5: Finding Infrequent (Non-Prevalent) Patterns ---\n",
        "print(\"\\n--- Phase 5: Running Infrequent Pattern Mining for different thresholds ---\")\n",
        "\n",
        "# List to collect all infrequent patterns across ALL distance, PI, and PR combinations\n",
        "all_infrequent_patterns_combined_list = []\n",
        "\n",
        "# Outer loop for distance thresholds\n",
        "for current_distance_threshold_m in distance_threshold_m:\n",
        "    print(f\"\\n===== Running Infrequent Pattern Mining for Distance: {current_distance_threshold_m}m =====\")\n",
        "\n",
        "    # Recalculate neighbors_map for the current distance threshold\n",
        "    neighbors_map = build_neighborhood_graph(current_distance_threshold_m, all_features_gdf, coords, tree)\n",
        "\n",
        "    # Nested loops to iterate through all PI and PR combinations\n",
        "    for current_max_pr in pr_thresholds: # Renamed to current_max_pr for clarity\n",
        "        for current_max_pi in pi_thresholds: # Renamed to current_max_pi for clarity\n",
        "            print(f\"\\n--- Current Thresholds (D={current_distance_threshold_m}m): Max PI={current_max_pi:.2f}, Max PR={current_max_pr:.2f} (for infrequent patterns) ---\")\n",
        "\n",
        "            infrequent_patterns_current_run = {\n",
        "                2: {},\n",
        "                3: {},\n",
        "                4: {}\n",
        "            }\n",
        "\n",
        "            # --- Generate and check all 2-item candidates ---\n",
        "            # For infrequent patterns, we need to check ALL combinations, not just Apriori-derived ones.\n",
        "            # Ensures we only consider existing feature types\n",
        "            filtered_unique_feature_types = [\n",
        "                f_type for f_type in unique_feature_types\n",
        "                if f_type in all_features_gdf['feature_type'].unique()\n",
        "            ]\n",
        "            candidate_2_item_patterns = list(itertools.combinations(filtered_unique_feature_types, 2))\n",
        "            print(f\"   Checking {len(candidate_2_item_patterns)} total 2-item candidate patterns...\")\n",
        "\n",
        "            for F1_type, F2_type in candidate_2_item_patterns:\n",
        "                pattern_key = frozenset([F1_type, F2_type])\n",
        "                pattern_pi, pattern_pr = calculate_pattern_prevalence(\n",
        "                    pattern_key, neighbors_map, all_features_gdf\n",
        "                )\n",
        "\n",
        "                # --- CORRECTED CONDITION FOR INFREQUENT PATTERNS ---\n",
        "                # A pattern is infrequent if it is NOT prevalent\n",
        "                # (i.e., its PI is below max_pi OR its PR is below max_pr)\n",
        "                if not (pattern_pi >= current_max_pi and pattern_pr >= current_max_pr):\n",
        "                    infrequent_patterns_current_run[2][pattern_key] = {'pi': pattern_pi, 'pr': pattern_pr}\n",
        "            print(f\"   Total infrequent 2-item patterns found: {len(infrequent_patterns_current_run[2])}\")\n",
        "\n",
        "\n",
        "            # --- Generate and check all 3-item candidates ---\n",
        "            print(\"\\n   Checking all 3-item candidate patterns...\")\n",
        "            candidate_3_item_patterns = list(itertools.combinations(filtered_unique_feature_types, 3))\n",
        "\n",
        "            for pattern_types in candidate_3_item_patterns:\n",
        "                pattern_key = frozenset(pattern_types)\n",
        "                pattern_pi, pattern_pr = calculate_pattern_prevalence(\n",
        "                    pattern_key, neighbors_map, all_features_gdf\n",
        "                )\n",
        "\n",
        "                if not (pattern_pi >= current_max_pi and pattern_pr >= current_max_pr):\n",
        "                    infrequent_patterns_current_run[3][pattern_key] = {'pi': pattern_pi, 'pr': pattern_pr}\n",
        "            print(f\"   Total infrequent 3-item patterns found: {len(infrequent_patterns_current_run[3])}\")\n",
        "\n",
        "\n",
        "            # --- Generate and check all 4-item candidates ---\n",
        "            print(\"\\n   Checking all 4-item candidate patterns...\")\n",
        "            candidate_4_item_patterns = list(itertools.combinations(filtered_unique_feature_types, 4))\n",
        "\n",
        "            for pattern_types in candidate_4_item_patterns:\n",
        "                pattern_key = frozenset(pattern_types)\n",
        "                pattern_pi, pattern_pr = calculate_pattern_prevalence(\n",
        "                    pattern_key, neighbors_map, all_features_gdf\n",
        "                )\n",
        "\n",
        "                if not (pattern_pi >= current_max_pi and pattern_pr >= current_max_pr):\n",
        "                    infrequent_patterns_current_run[4][pattern_key] = {'pi': pattern_pi, 'pr': pattern_pr}\n",
        "            print(f\"   Total infrequent 4-item patterns found: {len(infrequent_patterns_current_run[4])}\")\n",
        "\n",
        "\n",
        "            # --- Summary and Saving for the Current Infrequent Run (PI/PR/Distance) ---\n",
        "            print(f\"\\n   --- Summary for D={current_distance_threshold_m}m, Max PI={current_max_pi:.2f}, Max PR={current_max_pr:.2f} (Infrequent) ---\")\n",
        "            summary_data_infrequent_current_run = []\n",
        "            found_any_infrequent_patterns_current_run = False\n",
        "\n",
        "            for k in sorted(infrequent_patterns_current_run.keys()):\n",
        "                if infrequent_patterns_current_run[k]:\n",
        "                    found_any_infrequent_patterns_current_run = True\n",
        "                    print(f\"\\n   Infrequent {k}-item Patterns:\")\n",
        "                    sorted_patterns = sorted(infrequent_patterns_current_run[k].items(), key=lambda item: item[1]['pi'], reverse=False) # Sort PI ascending for infrequent\n",
        "                    for pattern, metrics in sorted_patterns:\n",
        "                        pattern_set = set(pattern)\n",
        "                        print(f\"    Pattern: {pattern_set}, PI: {metrics['pi']:.4f}, PR: {metrics['pr']:.4f}\")\n",
        "\n",
        "                        summary_data_infrequent_current_run.append({\n",
        "                            'distance_threshold_m': current_distance_threshold_m,\n",
        "                            'max_pi_threshold': current_max_pi, # Renamed to max_pi_threshold for clarity\n",
        "                            'max_pr_threshold': current_max_pr, # Renamed to max_pr_threshold for clarity\n",
        "                            'pattern_length': k,\n",
        "                            'pattern': \", \".join(sorted(pattern_set)),\n",
        "                            'participation_index': metrics['pi'],\n",
        "                            'participation_ratio': metrics['pr']\n",
        "                        })\n",
        "\n",
        "            if not found_any_infrequent_patterns_current_run:\n",
        "                print(\"   No infrequent co-location patterns found for this combination of thresholds.\")\n",
        "\n",
        "            # Save individual infrequent patterns CSV\n",
        "            if summary_data_infrequent_current_run:\n",
        "                infrequent_patterns_df_current_run = pd.DataFrame(summary_data_infrequent_current_run)\n",
        "                infrequent_patterns_df_current_run = infrequent_patterns_df_current_run.sort_values(\n",
        "                    by=['distance_threshold_m', 'pattern_length', 'participation_index'],\n",
        "                    ascending=[True, True, True] # Sort PI ascending for infrequent\n",
        "                ).reset_index(drop=True)\n",
        "\n",
        "                formatted_dist_str = f\"{current_distance_threshold_m}m\"\n",
        "                formatted_pi_str = f\"{current_max_pi:.2f}\".replace('.', '')\n",
        "                formatted_pr_str = f\"{current_max_pr:.2f}\".replace('.', '')\n",
        "                output_filename = f\"infrequent_patterns_D{formatted_dist_str}_MaxPI{formatted_pi_str}_MaxPR{formatted_pr_str}.csv\"\n",
        "                output_filepath = os.path.join(output_dir_colocation, output_filename)\n",
        "\n",
        "                infrequent_patterns_df_current_run.to_csv(output_filepath, index=False)\n",
        "                print(f\"   Infrequent co-location pattern summary saved to: {output_filepath}\")\n",
        "            else:\n",
        "                print(\"   No infrequent patterns CSV file saved for this run as no patterns were found.\")\n",
        "\n",
        "            # Extend the list for the final combined CSV for ALL infrequent patterns\n",
        "            all_infrequent_patterns_combined_list.extend(summary_data_infrequent_current_run)\n",
        "\n",
        "\n",
        "# --- Final Combined CSV for Infrequent Patterns ---\n",
        "print(\"\\n--- Phase 6: Generating combined CSV file for all infrequent patterns ---\")\n",
        "\n",
        "if all_infrequent_patterns_combined_list:\n",
        "    final_infrequent_patterns_df = pd.DataFrame(all_infrequent_patterns_combined_list)\n",
        "    final_infrequent_patterns_df = final_infrequent_patterns_df.sort_values(\n",
        "        by=['distance_threshold_m', 'max_pi_threshold', 'max_pr_threshold', 'pattern_length', 'participation_index'],\n",
        "        ascending=[True, True, True, True, True] # Sort PI ascending for overall infrequent\n",
        "    ).reset_index(drop=True)\n",
        "\n",
        "    combined_infrequent_output_filepath = os.path.join(output_dir_colocation, \"all_infrequent_patterns_combined.csv\")\n",
        "    final_infrequent_patterns_df.to_csv(combined_infrequent_output_filepath, index=False)\n",
        "    print(f\"All infrequent patterns combined and saved to: {combined_infrequent_output_filepath}\")\n",
        "else:\n",
        "    print(\"No infrequent patterns found across all runs to combine.\")\n",
        "\n",
        "print(\"\\n--- Infrequent Pattern Mining Process Completed ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qE8xhU4aBaL-"
      },
      "outputs": [],
      "source": [
        "final_infrequent_patterns_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJk5SvDiCEJn"
      },
      "outputs": [],
      "source": [
        "filtered_df = final_infrequent_patterns_df[\n",
        "    final_infrequent_patterns_df['pattern'].apply(lambda x: 'diffusion_tube' in x) &\n",
        "    (final_infrequent_patterns_df['pattern_length'] == 2)]\n",
        "\n",
        "filtered_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbML7HqHZuyw"
      },
      "source": [
        "## Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AH4kTNWxnzpY"
      },
      "source": [
        "### size-2 prevelent pattern: diffusion tube and school"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XxVCTsjmI1U"
      },
      "outputs": [],
      "source": [
        "# --- 1. Define the Pattern and Distance Range of Interest ---\n",
        "TARGET_FEATURE_TYPE_A = 'school'\n",
        "TARGET_FEATURE_TYPE_B = 'diffusion_tube'\n",
        "\n",
        "# The distance threshold for co-location analysis (in meters)\n",
        "DISTANCE_THRESHOLD = 600\n",
        "\n",
        "print(f\"\\n--- Analyzing co-location between '{TARGET_FEATURE_TYPE_A}' and '{TARGET_FEATURE_TYPE_B}' ---\")\n",
        "print(f\"Using a distance threshold of {DISTANCE_THRESHOLD}m.\")\n",
        "\n",
        "# --- 2. Load and Reproject Data (Assumed to be pre-loaded) ---\n",
        "# Reproject for the plotting part later\n",
        "all_features_gdf_wgs84 = all_features_gdf.to_crs(\"EPSG:4326\")\n",
        "\n",
        "# Extract GeoDataFrames for the target feature types\n",
        "gdf_type_A = all_features_gdf[all_features_gdf['feature_type'] == TARGET_FEATURE_TYPE_A].copy()\n",
        "gdf_type_B = all_features_gdf[all_features_gdf['feature_type'] == TARGET_FEATURE_TYPE_B].copy()\n",
        "\n",
        "if gdf_type_A.empty or gdf_type_B.empty:\n",
        "    print(f\"Error: One or both target feature types ('{TARGET_FEATURE_TYPE_A}', '{TARGET_FEATURE_TYPE_B}') not found.\")\n",
        "    exit()\n",
        "\n",
        "# Prepare coordinates for cKDTree for TARGET_FEATURE_TYPE_B (the \"nearest to\" feature)\n",
        "coords_B = np.array([[p.x, p.y] for p in gdf_type_B.geometry])\n",
        "tree_B = cKDTree(coords_B)\n",
        "\n",
        "# --- 3. Calculate Co-location Instances within the Distance Threshold ---\n",
        "co_location_pairs = []\n",
        "print(f\"Calculating co-location instances within {DISTANCE_THRESHOLD}m...\")\n",
        "\n",
        "for idx_A, row_A in gdf_type_A.iterrows():\n",
        "    point_A = row_A.geometry\n",
        "    # Query the KDTree to find the nearest neighbor within the threshold\n",
        "    distance, index = tree_B.query(np.array([point_A.x, point_A.y]), k=1, distance_upper_bound=DISTANCE_THRESHOLD)\n",
        "\n",
        "    # Check if a neighbor was found\n",
        "    if np.isfinite(distance):\n",
        "        co_location_pairs.append({\n",
        "            'source_uid': row_A['uid'],\n",
        "            'source_type': row_A['feature_type'],\n",
        "            'target_uid': gdf_type_B.iloc[index]['uid'],\n",
        "            'target_type': gdf_type_B.iloc[index]['feature_type'],\n",
        "            'distance_m': distance\n",
        "        })\n",
        "\n",
        "if not co_location_pairs:\n",
        "    print(f\"\\nNo co-location instances found between '{TARGET_FEATURE_TYPE_A}' and '{TARGET_FEATURE_TYPE_B}' within {DISTANCE_THRESHOLD}m.\")\n",
        "    # Exit or continue with a map that has no lines\n",
        "else:\n",
        "    # Create a DataFrame from the valid pairs for easy plotting\n",
        "    df_plot_lines = pd.DataFrame(co_location_pairs)\n",
        "    print(f\"\\nFound {len(df_plot_lines)} co-location pairs to plot.\")\n",
        "\n",
        "# --- 4. Plotting the Result on a Folium Map ---\n",
        "print(\"\\nGenerating Folium map to visualize all locations...\")\n",
        "\n",
        "# Reproject GeoDataFrames for plotting on Folium\n",
        "gdf_type_A_wgs84 = all_features_gdf_wgs84[all_features_gdf_wgs84['feature_type'] == TARGET_FEATURE_TYPE_A].copy()\n",
        "gdf_type_B_wgs84 = all_features_gdf_wgs84[all_features_gdf_wgs84['feature_type'] == TARGET_FEATURE_TYPE_B].copy()\n",
        "\n",
        "# Load and reproject boundaries\n",
        "sheffield_city_boundary_wgs84 = gpd.read_file(\"City_Boundary.geojson\").to_crs(\"EPSG:4326\")\n",
        "sheffield_ward_boundary_wgs84 = gpd.read_file(\"Sheffield_Wards.geojson\").to_crs(\"EPSG:4326\")\n",
        "\n",
        "# Create a Folium map object\n",
        "sheffield_center = [53.3810, -1.4701]\n",
        "m_prevalent_1 = folium.Map(location=sheffield_center, zoom_start=12)\n",
        "\n",
        "# Add boundaries to the map\n",
        "folium.GeoJson(sheffield_city_boundary_wgs84, name=\"Sheffield City Boundary\", style_function=lambda x: {'color': 'black', 'weight': 2, 'fillColor': 'none'}).add_to(m_prevalent_1)\n",
        "folium.GeoJson(sheffield_ward_boundary_wgs84, name=\"Sheffield Ward Boundary\", style_function=lambda x: {'color': 'black', 'weight': 1, 'fillColor': 'none'}).add_to(m_prevalent_1)\n",
        "\n",
        "# Create a feature group for the connecting lines\n",
        "lines_group = folium.FeatureGroup(name=f'Co-location Pairs ({DISTANCE_THRESHOLD}m)').add_to(m_prevalent_1)\n",
        "\n",
        "# Plot all schools and diffusion tubes that are part of the co-location pairs\n",
        "if 'df_plot_lines' in locals():\n",
        "    # Get the UIDs of the features that participate in the pairs\n",
        "    source_uids = df_plot_lines['source_uid'].unique()\n",
        "    target_uids = df_plot_lines['target_uid'].unique()\n",
        "\n",
        "    # Plot schools (Feature A)\n",
        "    schools_group = folium.FeatureGroup(name=f\"{TARGET_FEATURE_TYPE_A.title()} Locations\").add_to(m_prevalent_1)\n",
        "    for _, row in gdf_type_A_wgs84[gdf_type_A_wgs84['uid'].isin(source_uids)].iterrows():\n",
        "        folium.CircleMarker(\n",
        "            location=[row.geometry.y, row.geometry.x],\n",
        "            radius=5,\n",
        "            color='orange',\n",
        "            fill=True,\n",
        "            fill_color='orange',\n",
        "            fill_opacity=0.8,\n",
        "            popup=f\"{TARGET_FEATURE_TYPE_A.title()} (UID: {row['uid']})\"\n",
        "        ).add_to(schools_group)\n",
        "\n",
        "    # Plot diffusion tubes (Feature B)\n",
        "    diffusion_tube_group = folium.FeatureGroup(name=f\"{TARGET_FEATURE_TYPE_B.title()} Locations\").add_to(m_prevalent_1)\n",
        "    for _, row in gdf_type_B_wgs84[gdf_type_B_wgs84['uid'].isin(target_uids)].iterrows():\n",
        "        folium.CircleMarker(\n",
        "            location=[row.geometry.y, row.geometry.x],\n",
        "            radius=5,\n",
        "            color='blue',\n",
        "            fill=True,\n",
        "            fill_color='blue',\n",
        "            fill_opacity=0.8,\n",
        "            popup=f\"{TARGET_FEATURE_TYPE_B.title()} (UID: {row['uid']})\"\n",
        "        ).add_to(diffusion_tube_group)\n",
        "\n",
        "    # Draw the connecting lines\n",
        "    for _, row in df_plot_lines.iterrows():\n",
        "        source_coords = gdf_type_A_wgs84[gdf_type_A_wgs84['uid'] == row['source_uid']].geometry.iloc[0]\n",
        "        target_coords = gdf_type_B_wgs84[gdf_type_B_wgs84['uid'] == row['target_uid']].geometry.iloc[0]\n",
        "\n",
        "        folium.PolyLine(\n",
        "            locations=[[source_coords.y, source_coords.x], [target_coords.y, target_coords.x]],\n",
        "            color='red',\n",
        "            weight=2,\n",
        "            opacity=1,\n",
        "            dash_array='5, 5',\n",
        "            tooltip=f\"Distance: {row['distance_m']:.2f}m\"\n",
        "        ).add_to(lines_group)\n",
        "else:\n",
        "    print(\"No co-location pairs to plot. The map will show boundaries only.\")\n",
        "\n",
        "# Add a Layer Control to toggle layers on/off\n",
        "folium.LayerControl().add_to(m_prevalent_1)\n",
        "\n",
        "# Save the map as an HTML file\n",
        "map_filename = \"size-2_prevalent_school_diffusion_tube_map.html\"\n",
        "m_prevalent_1.save(map_filename)\n",
        "print(f\"\\nInteractive map saved to: {map_filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGp6oa3znsyC"
      },
      "outputs": [],
      "source": [
        "m_prevalent_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zloFq7Wr2WrI"
      },
      "outputs": [],
      "source": [
        "# --- 1. Define the Pattern and Distance Range of Interest ---\n",
        "TARGET_FEATURE_TYPE_A = 'school'\n",
        "TARGET_FEATURE_TYPE_B = 'diffusion_tube'\n",
        "\n",
        "# The distance threshold for co-location analysis (in meters)\n",
        "DISTANCE_THRESHOLD = 600\n",
        "\n",
        "print(f\"\\n--- Analyzing co-location between '{TARGET_FEATURE_TYPE_A}' and '{TARGET_FEATURE_TYPE_B}' ---\")\n",
        "print(f\"Using a distance threshold of {DISTANCE_THRESHOLD}m.\")\n",
        "\n",
        "# --- 2. Load and Reproject Data (Assumed to be pre-loaded) ---\n",
        "# Reproject for the plotting part later\n",
        "all_features_gdf_wgs84 = all_features_gdf.to_crs(\"EPSG:4326\")\n",
        "\n",
        "# Extract GeoDataFrames for the target feature types\n",
        "gdf_type_A = all_features_gdf[all_features_gdf['feature_type'] == TARGET_FEATURE_TYPE_A].copy()\n",
        "gdf_type_B = all_features_gdf[all_features_gdf['feature_type'] == TARGET_FEATURE_TYPE_B].copy()\n",
        "\n",
        "if gdf_type_A.empty or gdf_type_B.empty:\n",
        "    print(f\"Error: One or both target feature types ('{TARGET_FEATURE_TYPE_A}', '{TARGET_FEATURE_TYPE_B}') not found.\")\n",
        "    # In a real script, you might raise an error or exit here.\n",
        "    # For this example, we'll continue to generate a map with no lines.\n",
        "\n",
        "# Prepare coordinates for cKDTree for TARGET_FEATURE_TYPE_B (the \"nearest to\" feature)\n",
        "# The tree should be built on the geometry of the CRS that the distance is calculated in (e.g., BNG)\n",
        "# For this example, we'll assume the provided lon/lat are sufficient.\n",
        "coords_B = np.array([[p.x, p.y] for p in gdf_type_B.geometry])\n",
        "tree_B = cKDTree(coords_B)\n",
        "\n",
        "# --- 3. Calculate Co-location Instances within the Distance Threshold ---\n",
        "co_location_pairs = []\n",
        "print(f\"Calculating co-location instances within {DISTANCE_THRESHOLD}m...\")\n",
        "\n",
        "for idx_A, row_A in gdf_type_A.iterrows():\n",
        "    point_A = row_A.geometry\n",
        "    # Query the KDTree to find the nearest neighbor within the threshold\n",
        "    distance, index = tree_B.query(np.array([point_A.x, point_A.y]), k=1, distance_upper_bound=DISTANCE_THRESHOLD)\n",
        "\n",
        "    # Check if a neighbor was found\n",
        "    if np.isfinite(distance):\n",
        "        co_location_pairs.append({\n",
        "            'source_uid': row_A['uid'],\n",
        "            'source_type': row_A['feature_type'],\n",
        "            'target_uid': gdf_type_B.iloc[index]['uid'],\n",
        "            'target_type': gdf_type_B.iloc[index]['feature_type'],\n",
        "            'distance_m': distance\n",
        "        })\n",
        "\n",
        "if not co_location_pairs:\n",
        "    print(f\"\\nNo co-location instances found between '{TARGET_FEATURE_TYPE_A}' and '{TARGET_FEATURE_TYPE_B}' within {DISTANCE_THRESHOLD}m.\")\n",
        "    df_plot_lines = pd.DataFrame() # Create an empty DataFrame\n",
        "else:\n",
        "    # Create a DataFrame from the valid pairs for easy plotting\n",
        "    df_plot_lines = pd.DataFrame(co_location_pairs)\n",
        "    print(f\"\\nFound {len(df_plot_lines)} co-location pairs to plot.\")\n",
        "\n",
        "# --- 4. Plotting the Result on a Folium Map ---\n",
        "print(\"\\nGenerating Folium map to visualize all locations...\")\n",
        "\n",
        "# Reproject GeoDataFrames for plotting on Folium\n",
        "gdf_type_A_wgs84 = all_features_gdf_wgs84[all_features_gdf_wgs84['feature_type'] == TARGET_FEATURE_TYPE_A].copy()\n",
        "gdf_type_B_wgs84 = all_features_gdf_wgs84[all_features_gdf_wgs84['feature_type'] == TARGET_FEATURE_TYPE_B].copy()\n",
        "\n",
        "# Get the UIDs of the features that participate in the pairs\n",
        "source_uids = df_plot_lines['source_uid'].unique() if not df_plot_lines.empty else []\n",
        "target_uids = df_plot_lines['target_uid'].unique() if not df_plot_lines.empty else []\n",
        "\n",
        "# Load and reproject boundaries (assuming files exist)\n",
        "try:\n",
        "    sheffield_city_boundary_wgs84 = gpd.read_file(\"City_Boundary.geojson\").to_crs(\"EPSG:4326\")\n",
        "    sheffield_ward_boundary_wgs84 = gpd.read_file(\"Sheffield_Wards.geojson\").to_crs(\"EPSG:4326\")\n",
        "except Exception as e:\n",
        "    print(f\"Warning: Could not load boundary files. {e}\")\n",
        "    sheffield_city_boundary_wgs84 = gpd.GeoDataFrame()\n",
        "    sheffield_ward_boundary_wgs84 = gpd.GeoDataFrame()\n",
        "\n",
        "# Create a Folium map object\n",
        "sheffield_center = [53.3810, -1.4701]\n",
        "m_prevalent_1 = folium.Map(location=sheffield_center, zoom_start=12)\n",
        "\n",
        "# Add boundaries to the map\n",
        "if not sheffield_city_boundary_wgs84.empty:\n",
        "    folium.GeoJson(sheffield_city_boundary_wgs84, name=\"Sheffield City Boundary\", style_function=lambda x: {'color': 'black', 'weight': 2, 'fillColor': 'none'}).add_to(m_prevalent_1)\n",
        "if not sheffield_ward_boundary_wgs84.empty:\n",
        "    folium.GeoJson(sheffield_ward_boundary_wgs84, name=\"Sheffield Ward Boundary\", style_function=lambda x: {'color': 'black', 'weight': 1, 'fillColor': 'none'}).add_to(m_prevalent_1)\n",
        "\n",
        "# Create a feature group for the connecting lines\n",
        "lines_group = folium.FeatureGroup(name=f'Co-location Pairs ({DISTANCE_THRESHOLD}m)').add_to(m_prevalent_1)\n",
        "\n",
        "# Plot ALL schools and diffusion tubes, distinguishing paired from non-paired\n",
        "schools_group = folium.FeatureGroup(name=f\"{TARGET_FEATURE_TYPE_A.title()} Locations\").add_to(m_prevalent_1)\n",
        "for _, row in gdf_type_A_wgs84.iterrows():\n",
        "    # --- EDITED: Changed the color based on whether the school is a source in a pair ---\n",
        "    color = 'orange' if row['uid'] in source_uids else 'gray'\n",
        "    folium.CircleMarker(\n",
        "        location=[row.geometry.y, row.geometry.x],\n",
        "        radius=5,\n",
        "        color=color,\n",
        "        fill=True,\n",
        "        fill_color=color,\n",
        "        fill_opacity=0.8,\n",
        "        popup=f\"{TARGET_FEATURE_TYPE_A.title()} (UID: {row['uid']})\"\n",
        "    ).add_to(schools_group)\n",
        "\n",
        "diffusion_tube_group = folium.FeatureGroup(name=f\"{TARGET_FEATURE_TYPE_B.title()} Locations\").add_to(m_prevalent_1)\n",
        "for _, row in gdf_type_B_wgs84.iterrows():\n",
        "    # --- EDITED: Changed the color based on whether the diffusion tube is a target in a pair ---\n",
        "    color = 'blue' if row['uid'] in target_uids else 'lightgray'\n",
        "    folium.CircleMarker(\n",
        "        location=[row.geometry.y, row.geometry.x],\n",
        "        radius=5,\n",
        "        color=color,\n",
        "        fill=True,\n",
        "        fill_color=color,\n",
        "        fill_opacity=0.8,\n",
        "        popup=f\"{TARGET_FEATURE_TYPE_B.title()} (UID: {row['uid']})\"\n",
        "    ).add_to(diffusion_tube_group)\n",
        "\n",
        "# Draw the connecting lines\n",
        "if not df_plot_lines.empty:\n",
        "    for _, row in df_plot_lines.iterrows():\n",
        "        source_coords = gdf_type_A_wgs84[gdf_type_A_wgs84['uid'] == row['source_uid']].geometry.iloc[0]\n",
        "        target_coords = gdf_type_B_wgs84[gdf_type_B_wgs84['uid'] == row['target_uid']].geometry.iloc[0]\n",
        "        folium.PolyLine(\n",
        "            locations=[[source_coords.y, source_coords.x], [target_coords.y, target_coords.x]],\n",
        "            color='red',\n",
        "            weight=2,\n",
        "            opacity=1,\n",
        "            dash_array='5, 5',\n",
        "            tooltip=f\"Distance: {row['distance_m']:.2f}m\"\n",
        "        ).add_to(lines_group)\n",
        "\n",
        "# Add a Layer Control to toggle layers on/off\n",
        "folium.LayerControl().add_to(m_prevalent_1)\n",
        "\n",
        "# Save the map as an HTML file\n",
        "map_filename = \"size-2_prevalent_school_diffusion_tube_map.html\"\n",
        "m_prevalent_1.save(map_filename)\n",
        "print(f\"\\nInteractive map saved to: {map_filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVaWN4YHo1kz"
      },
      "source": [
        "### size-3 prevalent pattern: diffusion tube, school, and kindergarten"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3_HRKIIo8-l"
      },
      "outputs": [],
      "source": [
        "import folium\n",
        "import geopandas as gpd\n",
        "import numpy as np\n",
        "from scipy.spatial import cKDTree\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# --- 1. Define the Patterns and Distance Range of Interest ---\n",
        "TARGET_FEATURE_TYPE_A = 'school'\n",
        "TARGET_FEATURE_TYPE_B = 'diffusion_tube'\n",
        "TARGET_FEATURE_TYPE_C = 'kindergarten' # NEW: Added kindergarten as a target feature\n",
        "\n",
        "# The distance threshold for co-location analysis (in meters)\n",
        "DISTANCE_THRESHOLD = 600\n",
        "\n",
        "print(f\"\\n--- Analyzing co-location between '{TARGET_FEATURE_TYPE_A}', '{TARGET_FEATURE_TYPE_C}' and '{TARGET_FEATURE_TYPE_B}' ---\")\n",
        "print(f\"Using a distance threshold of {DISTANCE_THRESHOLD}m.\")\n",
        "\n",
        "# --- 2. Load and Reproject Data (Assumed to be pre-loaded) ---\n",
        "\n",
        "# Reproject for the plotting part later\n",
        "all_features_gdf_wgs84 = all_features_gdf.to_crs(\"EPSG:4326\")\n",
        "\n",
        "# Extract GeoDataFrames for the target feature types\n",
        "gdf_type_A = all_features_gdf[all_features_gdf['feature_type'] == TARGET_FEATURE_TYPE_A].copy()\n",
        "gdf_type_B = all_features_gdf[all_features_gdf['feature_type'] == TARGET_FEATURE_TYPE_B].copy()\n",
        "gdf_type_C = all_features_gdf[all_features_gdf['feature_type'] == TARGET_FEATURE_TYPE_C].copy() # NEW: DataFrame for kindergartens\n",
        "\n",
        "if gdf_type_A.empty or gdf_type_B.empty or gdf_type_C.empty:\n",
        "    print(\"Error: One or more target feature types not found.\")\n",
        "    exit()\n",
        "\n",
        "# Prepare coordinates for cKDTree for TARGET_FEATURE_TYPE_B (the \"nearest to\" feature)\n",
        "coords_B = np.array([[p.x, p.y] for p in gdf_type_B.geometry])\n",
        "tree_B = cKDTree(coords_B)\n",
        "\n",
        "# --- 3. Calculate Co-location Instances within the Distance Threshold ---\n",
        "co_location_pairs = []\n",
        "print(f\"Calculating co-location instances within {DISTANCE_THRESHOLD}m...\")\n",
        "\n",
        "# Loop for schools (Type A)\n",
        "for idx_A, row_A in gdf_type_A.iterrows():\n",
        "    point_A = row_A.geometry\n",
        "    distance, index = tree_B.query(np.array([point_A.x, point_A.y]), k=1, distance_upper_bound=DISTANCE_THRESHOLD)\n",
        "    if np.isfinite(distance):\n",
        "        co_location_pairs.append({\n",
        "            'source_uid': row_A['uid'], 'source_type': row_A['feature_type'],\n",
        "            'target_uid': gdf_type_B.iloc[index]['uid'], 'target_type': gdf_type_B.iloc[index]['feature_type'],\n",
        "            'distance_m': distance\n",
        "        })\n",
        "\n",
        "# NEW: Loop for kindergartens (Type C)\n",
        "for idx_C, row_C in gdf_type_C.iterrows():\n",
        "    point_C = row_C.geometry\n",
        "    distance, index = tree_B.query(np.array([point_C.x, point_C.y]), k=1, distance_upper_bound=DISTANCE_THRESHOLD)\n",
        "    if np.isfinite(distance):\n",
        "        co_location_pairs.append({\n",
        "            'source_uid': row_C['uid'], 'source_type': row_C['feature_type'],\n",
        "            'target_uid': gdf_type_B.iloc[index]['uid'], 'target_type': gdf_type_B.iloc[index]['feature_type'],\n",
        "            'distance_m': distance\n",
        "        })\n",
        "\n",
        "\n",
        "if not co_location_pairs:\n",
        "    print(f\"\\nNo co-location instances found between target features and '{TARGET_FEATURE_TYPE_B}' within {DISTANCE_THRESHOLD}m.\")\n",
        "else:\n",
        "    df_plot_lines = pd.DataFrame(co_location_pairs)\n",
        "    print(f\"\\nFound {len(df_plot_lines)} co-location pairs to plot.\")\n",
        "\n",
        "# --- 4. Plotting the Result on a Folium Map ---\n",
        "print(\"\\nGenerating Folium map to visualize all locations...\")\n",
        "\n",
        "# Reproject GeoDataFrames for plotting on Folium\n",
        "gdf_type_A_wgs84 = all_features_gdf_wgs84[all_features_gdf_wgs84['feature_type'] == TARGET_FEATURE_TYPE_A].copy()\n",
        "gdf_type_B_wgs84 = all_features_gdf_wgs84[all_features_gdf_wgs84['feature_type'] == TARGET_FEATURE_TYPE_B].copy()\n",
        "gdf_type_C_wgs84 = all_features_gdf_wgs84[all_features_gdf_wgs84['feature_type'] == TARGET_FEATURE_TYPE_C].copy() # NEW: DataFrame for plotting kindergartens\n",
        "\n",
        "# Load and reproject boundaries\n",
        "sheffield_city_boundary_wgs84 = gpd.read_file(\"City_Boundary.geojson\").to_crs(\"EPSG:4326\")\n",
        "sheffield_ward_boundary_wgs84 = gpd.read_file(\"Sheffield_Wards.geojson\").to_crs(\"EPSG:4326\")\n",
        "\n",
        "# Create a Folium map object\n",
        "sheffield_center = [53.3810, -1.4701]\n",
        "m_prevalent_2 = folium.Map(location=sheffield_center, zoom_start=12)\n",
        "\n",
        "# Add boundaries to the map\n",
        "folium.GeoJson(sheffield_city_boundary_wgs84, name=\"Sheffield City Boundary\", style_function=lambda x: {'color': 'black', 'weight': 2, 'fillColor': 'none'}).add_to(m_prevalent_2)\n",
        "folium.GeoJson(sheffield_ward_boundary_wgs84, name=\"Sheffield Ward Boundary\", style_function=lambda x: {'color': 'black', 'weight': 1, 'fillColor': 'none'}).add_to(m_prevalent_2)\n",
        "\n",
        "# Create a feature group for the connecting lines\n",
        "lines_group = folium.FeatureGroup(name=f'Co-location Pairs ({DISTANCE_THRESHOLD}m)').add_to(m_prevalent_2)\n",
        "\n",
        "if 'df_plot_lines' in locals():\n",
        "    # Plot schools (Feature A)\n",
        "    schools_group = folium.FeatureGroup(name=f\"{TARGET_FEATURE_TYPE_A.title()} Locations\").add_to(m_prevalent_2)\n",
        "    for _, row in gdf_type_A_wgs84.iterrows():\n",
        "        folium.CircleMarker(location=[row.geometry.y, row.geometry.x], radius=5, color='orange', fill=True, fill_color='orange', fill_opacity=0.8, popup=f\"{TARGET_FEATURE_TYPE_A.title()} (UID: {row['uid']})\").add_to(schools_group)\n",
        "\n",
        "    # Plot kindergartens (Feature C) - NEW\n",
        "    kindergarten_group = folium.FeatureGroup(name=f\"{TARGET_FEATURE_TYPE_C.title()} Locations\").add_to(m_prevalent_2)\n",
        "    for _, row in gdf_type_C_wgs84.iterrows():\n",
        "        folium.CircleMarker(location=[row.geometry.y, row.geometry.x], radius=5, color='purple', fill=True, fill_color='purple', fill_opacity=0.8, popup=f\"{TARGET_FEATURE_TYPE_C.title()} (UID: {row['uid']})\").add_to(kindergarten_group)\n",
        "\n",
        "    # Plot diffusion tubes (Feature B)\n",
        "    diffusion_tube_group = folium.FeatureGroup(name=f\"{TARGET_FEATURE_TYPE_B.title()} Locations\").add_to(m_prevalent_2)\n",
        "    for _, row in gdf_type_B_wgs84.iterrows():\n",
        "        folium.CircleMarker(location=[row.geometry.y, row.geometry.x], radius=5, color='blue', fill=True, fill_color='blue', fill_opacity=0.8, popup=f\"{TARGET_FEATURE_TYPE_B.title()} (UID: {row['uid']})\").add_to(diffusion_tube_group)\n",
        "\n",
        "    # Draw the connecting lines\n",
        "    for _, row in df_plot_lines.iterrows():\n",
        "        source_uid = row['source_uid']\n",
        "        source_type = row['source_type']\n",
        "        target_uid = row['target_uid']\n",
        "\n",
        "        # Get the coordinates for the source point\n",
        "        if source_type == TARGET_FEATURE_TYPE_A:\n",
        "            source_coords = gdf_type_A_wgs84[gdf_type_A_wgs84['uid'] == source_uid].geometry.iloc[0]\n",
        "        elif source_type == TARGET_FEATURE_TYPE_C: # NEW: Logic for kindergartens\n",
        "            source_coords = gdf_type_C_wgs84[gdf_type_C_wgs84['uid'] == source_uid].geometry.iloc[0]\n",
        "        else:\n",
        "            continue # Skip if source type is not a target type\n",
        "\n",
        "        target_coords = gdf_type_B_wgs84[gdf_type_B_wgs84['uid'] == target_uid].geometry.iloc[0]\n",
        "\n",
        "        # Determine line color based on the source feature\n",
        "        line_color = 'red' if source_type == TARGET_FEATURE_TYPE_A else 'purple' if source_type == TARGET_FEATURE_TYPE_C else 'green'\n",
        "\n",
        "        folium.PolyLine(\n",
        "            locations=[[source_coords.y, source_coords.x], [target_coords.y, target_coords.x]],\n",
        "            color=line_color,\n",
        "            weight=2,\n",
        "            opacity=1,\n",
        "            dash_array='5, 5',\n",
        "            tooltip=f\"Distance: {row['distance_m']:.2f}m\"\n",
        "        ).add_to(lines_group)\n",
        "else:\n",
        "    print(\"No co-location pairs to plot. The map will show boundaries only.\")\n",
        "\n",
        "# Add a Layer Control to toggle layers on/off\n",
        "folium.LayerControl().add_to(m_prevalent_2)\n",
        "\n",
        "# Save the map as an HTML file\n",
        "map_filename = \"size-3_prevalent_school_kindergarten_map.html\"\n",
        "m_prevalent_2.save(map_filename)\n",
        "print(f\"\\nInteractive map saved to: {map_filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R499R6MupGlw"
      },
      "outputs": [],
      "source": [
        "m_prevalent_2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJJ_UEW8vuUZ"
      },
      "source": [
        "### size-4 prevalent pattern: diffusion tube, school, kindergarten, and park"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7buJljsvtqA"
      },
      "outputs": [],
      "source": [
        "# --- 1. Define the Patterns and Distance Range of Interest ---\n",
        "TARGET_FEATURE_TYPE_A = 'school'\n",
        "TARGET_FEATURE_TYPE_B = 'diffusion_tube'\n",
        "TARGET_FEATURE_TYPE_C = 'kindergarten'\n",
        "TARGET_FEATURE_TYPE_D = 'park' # NEW: Added park as a target feature\n",
        "\n",
        "# The distance threshold for co-location analysis (in meters)\n",
        "DISTANCE_THRESHOLD = 600\n",
        "\n",
        "print(f\"\\n--- Analyzing co-location between '{TARGET_FEATURE_TYPE_A}', '{TARGET_FEATURE_TYPE_C}', '{TARGET_FEATURE_TYPE_D}' and '{TARGET_FEATURE_TYPE_B}' ---\")\n",
        "print(f\"Using a distance threshold of {DISTANCE_THRESHOLD}m.\")\n",
        "\n",
        "# --- 2. Load and Reproject Data (Assumed to be pre-loaded) ---\n",
        "\n",
        "# Reproject for the plotting part later\n",
        "all_features_gdf_wgs84 = all_features_gdf.to_crs(\"EPSG:4326\")\n",
        "\n",
        "# Extract GeoDataFrames for the target feature types\n",
        "gdf_type_A = all_features_gdf[all_features_gdf['feature_type'] == TARGET_FEATURE_TYPE_A].copy()\n",
        "gdf_type_B = all_features_gdf[all_features_gdf['feature_type'] == TARGET_FEATURE_TYPE_B].copy()\n",
        "gdf_type_C = all_features_gdf[all_features_gdf['feature_type'] == TARGET_FEATURE_TYPE_C].copy()\n",
        "gdf_type_D = all_features_gdf[all_features_gdf['feature_type'] == TARGET_FEATURE_TYPE_D].copy() # NEW: DataFrame for parks\n",
        "\n",
        "if gdf_type_A.empty or gdf_type_B.empty or gdf_type_C.empty or gdf_type_D.empty:\n",
        "    print(\"Error: One or more target feature types not found.\")\n",
        "    exit()\n",
        "\n",
        "# Prepare coordinates for cKDTree for TARGET_FEATURE_TYPE_B (the \"nearest to\" feature)\n",
        "coords_B = np.array([[p.x, p.y] for p in gdf_type_B.geometry])\n",
        "tree_B = cKDTree(coords_B)\n",
        "\n",
        "# --- 3. Calculate Co-location Instances within the Distance Threshold ---\n",
        "co_location_pairs = []\n",
        "print(f\"Calculating co-location instances within {DISTANCE_THRESHOLD}m...\")\n",
        "\n",
        "# Loop for schools (Type A)\n",
        "for idx_A, row_A in gdf_type_A.iterrows():\n",
        "    point_A = row_A.geometry\n",
        "    distance, index = tree_B.query(np.array([point_A.x, point_A.y]), k=1, distance_upper_bound=DISTANCE_THRESHOLD)\n",
        "    if np.isfinite(distance):\n",
        "        co_location_pairs.append({\n",
        "            'source_uid': row_A['uid'], 'source_type': row_A['feature_type'],\n",
        "            'target_uid': gdf_type_B.iloc[index]['uid'], 'target_type': gdf_type_B.iloc[index]['feature_type'],\n",
        "            'distance_m': distance\n",
        "        })\n",
        "\n",
        "# Loop for kindergartens (Type C)\n",
        "for idx_C, row_C in gdf_type_C.iterrows():\n",
        "    point_C = row_C.geometry\n",
        "    distance, index = tree_B.query(np.array([point_C.x, point_C.y]), k=1, distance_upper_bound=DISTANCE_THRESHOLD)\n",
        "    if np.isfinite(distance):\n",
        "        co_location_pairs.append({\n",
        "            'source_uid': row_C['uid'], 'source_type': row_C['feature_type'],\n",
        "            'target_uid': gdf_type_B.iloc[index]['uid'], 'target_type': gdf_type_B.iloc[index]['feature_type'],\n",
        "            'distance_m': distance\n",
        "        })\n",
        "\n",
        "# NEW: Loop for parks (Type D)\n",
        "for idx_D, row_D in gdf_type_D.iterrows():\n",
        "    point_D = row_D.geometry\n",
        "    distance, index = tree_B.query(np.array([point_D.x, point_D.y]), k=1, distance_upper_bound=DISTANCE_THRESHOLD)\n",
        "    if np.isfinite(distance):\n",
        "        co_location_pairs.append({\n",
        "            'source_uid': row_D['uid'], 'source_type': row_D['feature_type'],\n",
        "            'target_uid': gdf_type_B.iloc[index]['uid'], 'target_type': gdf_type_B.iloc[index]['feature_type'],\n",
        "            'distance_m': distance\n",
        "        })\n",
        "\n",
        "if not co_location_pairs:\n",
        "    print(f\"\\nNo co-location instances found between target features and '{TARGET_FEATURE_TYPE_B}' within {DISTANCE_THRESHOLD}m.\")\n",
        "else:\n",
        "    df_plot_lines = pd.DataFrame(co_location_pairs)\n",
        "    print(f\"\\nFound {len(df_plot_lines)} co-location pairs to plot.\")\n",
        "\n",
        "# --- 4. Plotting the Result on a Folium Map ---\n",
        "print(\"\\nGenerating Folium map to visualize all locations...\")\n",
        "\n",
        "# Reproject GeoDataFrames for plotting on Folium\n",
        "gdf_type_A_wgs84 = all_features_gdf_wgs84[all_features_gdf_wgs84['feature_type'] == TARGET_FEATURE_TYPE_A].copy()\n",
        "gdf_type_B_wgs84 = all_features_gdf_wgs84[all_features_gdf_wgs84['feature_type'] == TARGET_FEATURE_TYPE_B].copy()\n",
        "gdf_type_C_wgs84 = all_features_gdf_wgs84[all_features_gdf_wgs84['feature_type'] == TARGET_FEATURE_TYPE_C].copy()\n",
        "gdf_type_D_wgs84 = all_features_gdf_wgs84[all_features_gdf_wgs84['feature_type'] == TARGET_FEATURE_TYPE_D].copy() # NEW: DataFrame for plotting parks\n",
        "\n",
        "# Load and reproject boundaries\n",
        "sheffield_city_boundary_wgs84 = gpd.read_file(\"City_Boundary.geojson\").to_crs(\"EPSG:4326\")\n",
        "sheffield_ward_boundary_wgs84 = gpd.read_file(\"Sheffield_Wards.geojson\").to_crs(\"EPSG:4326\")\n",
        "\n",
        "# Create a Folium map object\n",
        "sheffield_center = [53.3810, -1.4701]\n",
        "m_prevalent_3 = folium.Map(location=sheffield_center, zoom_start=12)\n",
        "\n",
        "# Add boundaries to the map\n",
        "folium.GeoJson(sheffield_city_boundary_wgs84, name=\"Sheffield City Boundary\", style_function=lambda x: {'color': 'black', 'weight': 2, 'fillColor': 'none'}).add_to(m_prevalent_3)\n",
        "folium.GeoJson(sheffield_ward_boundary_wgs84, name=\"Sheffield Ward Boundary\", style_function=lambda x: {'color': 'black', 'weight': 1, 'fillColor': 'none'}).add_to(m_prevalent_3)\n",
        "\n",
        "# Create a feature group for the connecting lines\n",
        "lines_group = folium.FeatureGroup(name=f'Co-location Pairs ({DISTANCE_THRESHOLD}m)').add_to(m_prevalent_3)\n",
        "\n",
        "if 'df_plot_lines' in locals():\n",
        "    # Plot schools (Feature A)\n",
        "    schools_group = folium.FeatureGroup(name=f\"{TARGET_FEATURE_TYPE_A.title()} Locations\").add_to(m_prevalent_3)\n",
        "    for _, row in gdf_type_A_wgs84.iterrows():\n",
        "        folium.CircleMarker(location=[row.geometry.y, row.geometry.x], radius=5, color='orange', fill=True, fill_color='orange', fill_opacity=0.8, popup=f\"{TARGET_FEATURE_TYPE_A.title()} (UID: {row['uid']})\").add_to(schools_group)\n",
        "\n",
        "    # Plot kindergartens (Feature C)\n",
        "    kindergarten_group = folium.FeatureGroup(name=f\"{TARGET_FEATURE_TYPE_C.title()} Locations\").add_to(m_prevalent_3)\n",
        "    for _, row in gdf_type_C_wgs84.iterrows():\n",
        "        folium.CircleMarker(location=[row.geometry.y, row.geometry.x], radius=5, color='purple', fill=True, fill_color='purple', fill_opacity=0.8, popup=f\"{TARGET_FEATURE_TYPE_C.title()} (UID: {row['uid']})\").add_to(kindergarten_group)\n",
        "\n",
        "    # Plot parks (Feature D) - NEW\n",
        "    parks_group = folium.FeatureGroup(name=f\"{TARGET_FEATURE_TYPE_D.title()} Locations\").add_to(m_prevalent_3)\n",
        "    for _, row in gdf_type_D_wgs84.iterrows():\n",
        "        folium.CircleMarker(location=[row.geometry.y, row.geometry.x], radius=5, color='green', fill=True, fill_color='green', fill_opacity=0.8, popup=f\"{TARGET_FEATURE_TYPE_D.title()} (UID: {row['uid']})\").add_to(parks_group)\n",
        "\n",
        "    # Plot diffusion tubes (Feature B)\n",
        "    diffusion_tube_group = folium.FeatureGroup(name=f\"{TARGET_FEATURE_TYPE_B.title()} Locations\").add_to(m_prevalent_3)\n",
        "    for _, row in gdf_type_B_wgs84.iterrows():\n",
        "        folium.CircleMarker(location=[row.geometry.y, row.geometry.x], radius=5, color='blue', fill=True, fill_color='blue', fill_opacity=0.8, popup=f\"{TARGET_FEATURE_TYPE_B.title()} (UID: {row['uid']})\").add_to(diffusion_tube_group)\n",
        "\n",
        "    # Draw the connecting lines\n",
        "    for _, row in df_plot_lines.iterrows():\n",
        "        source_uid = row['source_uid']\n",
        "        source_type = row['source_type']\n",
        "        target_uid = row['target_uid']\n",
        "\n",
        "        # Get the coordinates for the source point\n",
        "        if source_type == TARGET_FEATURE_TYPE_A:\n",
        "            source_coords = gdf_type_A_wgs84[gdf_type_A_wgs84['uid'] == source_uid].geometry.iloc[0]\n",
        "        elif source_type == TARGET_FEATURE_TYPE_C:\n",
        "            source_coords = gdf_type_C_wgs84[gdf_type_C_wgs84['uid'] == source_uid].geometry.iloc[0]\n",
        "        elif source_type == TARGET_FEATURE_TYPE_D: # NEW: Logic for parks\n",
        "            source_coords = gdf_type_D_wgs84[gdf_type_D_wgs84['uid'] == source_uid].geometry.iloc[0]\n",
        "        else:\n",
        "            continue # Skip if source type is not a target type\n",
        "\n",
        "        target_coords = gdf_type_B_wgs84[gdf_type_B_wgs84['uid'] == target_uid].geometry.iloc[0]\n",
        "\n",
        "        # Determine line color based on the source feature\n",
        "        if source_type == TARGET_FEATURE_TYPE_A:\n",
        "            line_color = 'orange'\n",
        "        elif source_type == TARGET_FEATURE_TYPE_C:\n",
        "            line_color = 'purple'\n",
        "        elif source_type == TARGET_FEATURE_TYPE_D: # NEW: Line color for parks\n",
        "            line_color = 'green'\n",
        "        else:\n",
        "            line_color = 'red' # Default color\n",
        "\n",
        "        folium.PolyLine(\n",
        "            locations=[[source_coords.y, source_coords.x], [target_coords.y, target_coords.x]],\n",
        "            color=line_color,\n",
        "            weight=2,\n",
        "            opacity=1,\n",
        "            dash_array='5, 5',\n",
        "            tooltip=f\"Distance: {row['distance_m']:.2f}m\"\n",
        "        ).add_to(lines_group)\n",
        "else:\n",
        "    print(\"No co-location pairs to plot. The map will show boundaries only.\")\n",
        "\n",
        "# Add a Layer Control to toggle layers on/off\n",
        "folium.LayerControl().add_to(m_prevalent_3)\n",
        "\n",
        "# Save the map as an HTML file\n",
        "map_filename = \"size-4_prevalent_map.html\"\n",
        "m_prevalent_3.save(map_filename)\n",
        "print(f\"\\nInteractive map saved to: {map_filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h21lxznIwCkW"
      },
      "outputs": [],
      "source": [
        "m_prevalent_3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-s1WBEpYgCfi"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# Reproject to a local projected CRS (e.g., British National Grid) for accurate distance calculation\n",
        "local_crs = \"EPSG:27700\"\n",
        "all_features_gdf = all_features_gdf.to_crs(local_crs)\n",
        "\n",
        "# --- Define the Most Prevalent Pattern and its Colors ---\n",
        "MOST_PREVALENT_PATTERN = ['diffusion_tube', 'kindergarten', 'park', 'school']\n",
        "FEATURE_COLORS = {\n",
        "    'diffusion_tube': 'blue',\n",
        "    'kindergarten': 'purple',\n",
        "    'park': 'green',\n",
        "    'school': 'red'\n",
        "    }\n",
        "FEATURE_RADIUS = 3\n",
        "\n",
        "# The maximum distance to consider points part of the same cluster (in meters)\n",
        "CLUSTER_DISTANCE_THRESHOLD = 600\n",
        "MIN_POINTS_IN_CLUSTER = len(MOST_PREVALENT_PATTERN)\n",
        "\n",
        "# --- 2. Load and Reproject Boundaries for the Map ---\n",
        "print(\"\\nLoading and reprojecting boundaries...\")\n",
        "sheffield_city_boundary_wgs84 = gpd.read_file(\"City_Boundary.geojson\").to_crs(\"EPSG:4326\")\n",
        "sheffield_ward_boundary_wgs84 = gpd.read_file(\"Sheffield_Wards.geojson\").to_crs(\"EPSG:4326\")\n",
        "\n",
        "# Reproject all features for Folium plotting (back to WGS84)\n",
        "all_features_gdf_wgs84 = all_features_gdf.to_crs(\"EPSG:4326\")\n",
        "\n",
        "# --- 3. Create a Folium Map Object and Add Boundaries ---\n",
        "sheffield_center = [53.3810, -1.4701]\n",
        "m_prevalent_pattern = folium.Map(location=sheffield_center, zoom_start=12)\n",
        "\n",
        "folium.GeoJson(sheffield_city_boundary_wgs84, name=\"Sheffield City Boundary\", style_function=lambda x: {'color': 'black', 'weight': 2, 'fillColor': 'none'}).add_to(m_prevalent_pattern)\n",
        "folium.GeoJson(sheffield_ward_boundary_wgs84, name=\"Sheffield Ward Boundary\", style_function=lambda x: {'color': 'black', 'weight': 1, 'fillColor': 'none'}).add_to(m_prevalent_pattern)\n",
        "\n",
        "# --- 4. Identify and Plot Co-location Clusters as Polygons ---\n",
        "print(f\"Identifying and plotting clusters for the most prevalent pattern: {MOST_PREVALENT_PATTERN}\")\n",
        "\n",
        "# Filter the data for the prevalent pattern features\n",
        "prevalent_features_gdf = all_features_gdf_wgs84[all_features_gdf_wgs84['feature_type'].isin(MOST_PREVALENT_PATTERN)]\n",
        "prevalent_features_gdf_projected = prevalent_features_gdf.to_crs(local_crs)\n",
        "\n",
        "# Use DBSCAN to find clusters. A cluster is defined as a group of points\n",
        "# where each point is within `eps` distance of at least `min_samples` other points.\n",
        "coords = np.array([[p.x, p.y] for p in prevalent_features_gdf_projected.geometry])\n",
        "db = DBSCAN(eps=CLUSTER_DISTANCE_THRESHOLD, min_samples=MIN_POINTS_IN_CLUSTER).fit(coords)\n",
        "labels = db.labels_\n",
        "\n",
        "# Create a FeatureGroup for the cluster polygons\n",
        "cluster_group = folium.FeatureGroup(name=\"Co-location Clusters\").add_to(m_prevalent_pattern)\n",
        "cluster_id_counter = 0\n",
        "\n",
        "for cluster_id in set(labels):\n",
        "    if cluster_id != -1:  # Ignore noise points (outliers)\n",
        "        cluster_points_gdf = prevalent_features_gdf_projected[labels == cluster_id]\n",
        "\n",
        "        # We are looking for patterns with a mix of all four types.\n",
        "        # Check if this cluster contains all feature types in the prevalent pattern\n",
        "        cluster_feature_types = set(cluster_points_gdf['feature_type'])\n",
        "        if cluster_feature_types.issuperset(set(MOST_PREVALENT_PATTERN)):\n",
        "            # Create a convex hull from the cluster points\n",
        "            multi_point = MultiPoint(list(cluster_points_gdf.geometry))\n",
        "            cluster_polygon = multi_point.convex_hull\n",
        "\n",
        "            # Create a GeoSeries with the polygon and then reproject it\n",
        "            cluster_polygon_geoseries = gpd.GeoSeries(cluster_polygon, crs=local_crs)\n",
        "            cluster_polygon_wgs84 = cluster_polygon_geoseries.to_crs(\"EPSG:4326\")\n",
        "\n",
        "            # Plot the polygon on the map\n",
        "            folium.GeoJson(cluster_polygon_wgs84,\n",
        "                           name=f\"Cluster {cluster_id_counter}\",\n",
        "                           style_function=lambda x: {\n",
        "                               'color': 'darkorange',\n",
        "                               'weight': 2,\n",
        "                               'fillColor': 'orange',\n",
        "                               'fillOpacity': 0.2\n",
        "                               }\n",
        "                           ).add_to(cluster_group)\n",
        "\n",
        "# --- 5. Plot Each Feature Individually on Top of the Polygons ---\n",
        "for feature_type in MOST_PREVALENT_PATTERN:\n",
        "    gdf_filtered = all_features_gdf_wgs84[all_features_gdf_wgs84['feature_type'] == feature_type]\n",
        "\n",
        "    if not gdf_filtered.empty:\n",
        "        feature_group = folium.FeatureGroup(name=f\"{feature_type.title()} Locations\").add_to(m_prevalent_pattern)\n",
        "\n",
        "        for _, row in gdf_filtered.iterrows():\n",
        "            folium.CircleMarker(\n",
        "                location=[row.geometry.y, row.geometry.x],\n",
        "                radius=FEATURE_RADIUS,\n",
        "                color=FEATURE_COLORS[feature_type],\n",
        "                fill=True,\n",
        "                fill_color=FEATURE_COLORS[feature_type],\n",
        "                fill_opacity=0.8,\n",
        "                popup=f\"Type: {feature_type.title()}\"\n",
        "            ).add_to(feature_group)\n",
        "\n",
        "# Add a Layer Control to toggle layers on/off\n",
        "folium.LayerControl().add_to(m_prevalent_pattern)\n",
        "\n",
        "# Save the map as an HTML file\n",
        "map_filename = \"most_prevalent_pattern_map.html\"\n",
        "m_prevalent_pattern.save(map_filename)\n",
        "print(f\"\\nInteractive map saved to: {map_filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nc0j6SfNhQ3g"
      },
      "outputs": [],
      "source": [
        "m_prevalent_pattern"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f_to0vddjwb"
      },
      "source": [
        "### Non-prevalent pattern: diffusion tube and nursing home"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZVdZw3WXzIH"
      },
      "outputs": [],
      "source": [
        "# --- 1. Define the Pattern and Distance Range of Interest ---\n",
        "TARGET_FEATURE_TYPE_A = 'nursing_home'\n",
        "TARGET_FEATURE_TYPE_B = 'diffusion_tube'\n",
        "\n",
        "# The distance threshold for co-location analysis (in meters)\n",
        "DISTANCE_THRESHOLD = 200\n",
        "\n",
        "print(f\"\\n--- Analyzing co-location between '{TARGET_FEATURE_TYPE_A}' and '{TARGET_FEATURE_TYPE_B}' ---\")\n",
        "print(f\"Using a distance threshold of {DISTANCE_THRESHOLD}m.\")\n",
        "\n",
        "# --- 2. Load and Reproject Data (You must have these GeoDataFrames loaded) ---\n",
        "\n",
        "# Reproject to a local projected CRS (e.g., British National Grid) for accurate distance calculation\n",
        "all_features_gdf_wgs84 = all_features_gdf.to_crs(\"EPSG:4326\")\n",
        "\n",
        "# Extract GeoDataFrames for the target feature types\n",
        "gdf_type_A = all_features_gdf[all_features_gdf['feature_type'] == TARGET_FEATURE_TYPE_A].copy()\n",
        "gdf_type_B = all_features_gdf[all_features_gdf['feature_type'] == TARGET_FEATURE_TYPE_B].copy()\n",
        "\n",
        "if gdf_type_A.empty or gdf_type_B.empty:\n",
        "    print(f\"Error: One or both target feature types ('{TARGET_FEATURE_TYPE_A}', '{TARGET_FEATURE_TYPE_B}') not found.\")\n",
        "    exit()\n",
        "\n",
        "# Prepare coordinates for cKDTree for TARGET_FEATURE_TYPE_B (the \"nearest to\" feature)\n",
        "coords_B = np.array([[p.x, p.y] for p in gdf_type_B.geometry])\n",
        "tree_B = cKDTree(coords_B)\n",
        "\n",
        "# --- 3. Calculate Co-location Instances within the Distance Threshold ---\n",
        "co_location_pairs = []\n",
        "print(f\"Calculating co-location instances within {DISTANCE_THRESHOLD}m...\")\n",
        "\n",
        "for idx_A, row_A in gdf_type_A.iterrows():\n",
        "    point_A = row_A.geometry\n",
        "    # Query the KDTree to find the nearest neighbor within the threshold\n",
        "    distance, index = tree_B.query(np.array([point_A.x, point_A.y]), k=1, distance_upper_bound=DISTANCE_THRESHOLD)\n",
        "\n",
        "    if np.isfinite(distance):\n",
        "        # A co-location instance is found\n",
        "        co_location_pairs.append({\n",
        "            'source_uid': row_A['uid'],\n",
        "            'source_type': row_A['feature_type'],\n",
        "            'target_uid': gdf_type_B.iloc[index]['uid'],\n",
        "            'target_type': gdf_type_B.iloc[index]['feature_type'],\n",
        "            'distance_m': distance\n",
        "        })\n",
        "\n",
        "if not co_location_pairs:\n",
        "    print(f\"\\nNo co-location instances found between '{TARGET_FEATURE_TYPE_A}' and '{TARGET_FEATURE_TYPE_B}' within {DISTANCE_THRESHOLD}m.\")\n",
        "    print(\"This confirms the pattern has a low prevalence, including a PI of 0.\")\n",
        "else:\n",
        "    pass\n",
        "\n",
        "# --- 4. Plotting the Result on a Folium Map ---\n",
        "print(\"\\nGenerating Folium map to visualize all locations...\")\n",
        "\n",
        "# Reproject for Folium plotting (back to WGS84)\n",
        "all_features_gdf_wgs84 = all_features_gdf.to_crs(\"EPSG:4326\")\n",
        "gdf_type_A_wgs84 = all_features_gdf_wgs84[all_features_gdf_wgs84['feature_type'] == 'nursing_home'].copy()\n",
        "gdf_type_B_wgs84 = all_features_gdf_wgs84[all_features_gdf_wgs84['feature_type'] == 'diffusion_tube'].copy()\n",
        "\n",
        "# Load and reproject boundaries (assuming files are in the same directory)\n",
        "sheffield_city_boundary_wgs84 = gpd.read_file(\"City_Boundary.geojson\").to_crs(\"EPSG:4326\")\n",
        "sheffield_ward_boundary_wgs84 = gpd.read_file(\"Sheffield_Wards.geojson\").to_crs(\"EPSG:4326\")\n",
        "\n",
        "# Create a Folium map object\n",
        "sheffield_center = [53.3810, -1.4701]\n",
        "m_non_prevalent = folium.Map(location=sheffield_center, zoom_start=12)\n",
        "\n",
        "# Add boundaries to the map\n",
        "folium.GeoJson(sheffield_city_boundary_wgs84, name=\"Sheffield City Boundary\", style_function=lambda x: {'color': 'black', 'weight': 2, 'fillColor': 'none'}).add_to(m_non_prevalent)\n",
        "folium.GeoJson(sheffield_ward_boundary_wgs84, name=\"Sheffield Ward Boundary\", style_function=lambda x: {'color': 'black', 'weight': 1, 'fillColor': 'none'}).add_to(m_non_prevalent)\n",
        "\n",
        "# Create a feature group for the connecting lines\n",
        "lines_group = folium.FeatureGroup(name=f'Nearest Neighbors (for reference)').add_to(m_non_prevalent)\n",
        "\n",
        "# Plot all nursing homes (orange circles)\n",
        "nursing_home_group = folium.FeatureGroup(name=\"Nursing Homes\").add_to(m_non_prevalent)\n",
        "for _, row_wgs84 in gdf_type_A_wgs84.iterrows():\n",
        "    folium.CircleMarker(\n",
        "        location=[row_wgs84.geometry.y, row_wgs84.geometry.x],\n",
        "        radius=5,\n",
        "        color='orange',\n",
        "        fill=True,\n",
        "        fill_color='orange',\n",
        "        fill_opacity=0.8,\n",
        "        popup=f\"Nursing Home (UID: {row_wgs84['uid']})\"\n",
        "    ).add_to(nursing_home_group)\n",
        "\n",
        "    # To get the distance, you must query the KDTree built with the local CRS data\n",
        "    point_A_local = gdf_type_A[gdf_type_A['uid'] == row_wgs84['uid']].geometry.iloc[0]\n",
        "    distance, index = tree_B.query(np.array([point_A_local.x, point_A_local.y]), k=1)\n",
        "\n",
        "    # Get the nearest diffusion tube in WGS84 for plotting\n",
        "    nearest_diffusion_tube_wgs84 = gdf_type_B_wgs84.iloc[index]\n",
        "\n",
        "    # Draw a line to the nearest diffusion tube, regardless of the distance, for a complete picture\n",
        "    folium.PolyLine(\n",
        "        locations=[[row_wgs84.geometry.y, row_wgs84.geometry.x], [nearest_diffusion_tube_wgs84.geometry.y, nearest_diffusion_tube_wgs84.geometry.x]],\n",
        "        color='red',\n",
        "        weight=2,\n",
        "        opacity=1,\n",
        "        dash_array='5, 5',\n",
        "        tooltip=f\"Distance: {distance:.2f}m\"  # This is the line that adds the distance tooltip\n",
        "    ).add_to(lines_group)\n",
        "\n",
        "\n",
        "# Plot all diffusion tubes (blue circles)\n",
        "diffusion_tube_group = folium.FeatureGroup(name=\"Diffusion Tubes\").add_to(m_non_prevalent)\n",
        "for _, row in gdf_type_B_wgs84.iterrows():\n",
        "    folium.CircleMarker(\n",
        "        location=[row.geometry.y, row.geometry.x],\n",
        "        radius=5,\n",
        "        color='blue',\n",
        "        fill=True,\n",
        "        fill_color='blue',\n",
        "        fill_opacity=0.8,\n",
        "        popup=f\"Diffusion Tube (UID: {row['uid']})\"\n",
        "    ).add_to(diffusion_tube_group)\n",
        "\n",
        "# Add a Layer Control to toggle layers on/off\n",
        "folium.LayerControl().add_to(m_non_prevalent)\n",
        "\n",
        "# Save the map as an HTML file\n",
        "map_filename = \"non_prevalent_nursing_homes_map.html\"\n",
        "m_non_prevalent.save(map_filename)\n",
        "print(f\"\\nInteractive map saved to: {map_filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IA9aECD5ZXct"
      },
      "outputs": [],
      "source": [
        "m_non_prevalent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNDmbhOv0ry0"
      },
      "outputs": [],
      "source": [
        "# Define the path to your Sheffield boundary GeoJSON file\n",
        "SHEFFIELD_BOUNDARY_GEOJSON_PATH = \"/content/City_Boundary.geojson\"\n",
        "output_dir_colocation = \"colocation_data\"\n",
        "os.makedirs(output_dir_colocation, exist_ok=True)\n",
        "\n",
        "# --- 1. Define the Pattern and Distance Range of Interest ---\n",
        "TARGET_FEATURE_TYPE_A = 'nursing_home'\n",
        "TARGET_FEATURE_TYPE_B = 'diffusion_tube' # The feature type you want to find neighbors of\n",
        "\n",
        "# Define the distance range to filter by (in meters)\n",
        "DISTANCE_MIN_FOR_PLOT = 10 # Minimum distance to consider for plotting\n",
        "DISTANCE_MAX_FOR_PLOT = 400 # Maximum distance to consider for plotting\n",
        "\n",
        "print(f\"\\n--- Analyzing distances for '{TARGET_FEATURE_TYPE_A}' to nearest '{TARGET_FEATURE_TYPE_B}' ---\")\n",
        "print(f\"Filtering results for distances between {DISTANCE_MIN_FOR_PLOT}m and {DISTANCE_MAX_FOR_PLOT}m.\")\n",
        "\n",
        "# --- 2. Extract GeoDataFrames for the Target Feature Types (using WGS84 for Folium) ---\n",
        "\n",
        "new_crs = \"EPSG:4326\"   # WGS84 for Folium\n",
        "all_features_gdf_wgs84 = all_features_gdf.to_crs(new_crs)\n",
        "\n",
        "gdf_type_A_wgs84 = all_features_gdf_wgs84[all_features_gdf_wgs84['feature_type'] == TARGET_FEATURE_TYPE_A].copy()\n",
        "gdf_type_B_wgs84 = all_features_gdf_wgs84[all_features_gdf_wgs84['feature_type'] == TARGET_FEATURE_TYPE_B].copy()\n",
        "\n",
        "# Use original CRS data for KDTree query as distance calculations are done in projected CRS\n",
        "gdf_type_A_original_crs = all_features_gdf[all_features_gdf['feature_type'] == TARGET_FEATURE_TYPE_A].copy()\n",
        "gdf_type_B_original_crs = all_features_gdf[all_features_gdf['feature_type'] == TARGET_FEATURE_TYPE_B].copy()\n",
        "\n",
        "\n",
        "if gdf_type_A_wgs84.empty or gdf_type_B_wgs84.empty:\n",
        "    print(f\"Error: One or both target feature types ('{TARGET_FEATURE_TYPE_A}', '{TARGET_FEATURE_TYPE_B}') not found in your data.\")\n",
        "    exit()\n",
        "\n",
        "# Prepare coordinates for cKDTree for TARGET_FEATURE_TYPE_B (the \"nearest to\" feature)\n",
        "# IMPORTANT: Use original CRS for distance calculation with KDTree\n",
        "coords_B_original_crs = np.array([[p.x, p.y] for p in gdf_type_B_original_crs.geometry])\n",
        "tree_B_original_crs = cKDTree(coords_B_original_crs)\n",
        "\n",
        "\n",
        "# --- 3. Calculate Nearest Neighbor Distances and Prepare Plotting Data ---\n",
        "plot_data_lines = []\n",
        "\n",
        "print(f\"Calculating nearest '{TARGET_FEATURE_TYPE_B}' for each '{TARGET_FEATURE_TYPE_A}' instance...\")\n",
        "\n",
        "for idx_A, row_A in gdf_type_A_original_crs.iterrows(): # Iterate over original CRS for query\n",
        "    point_A_original_crs = row_A.geometry\n",
        "    uid_A = row_A['uid']\n",
        "    type_A = row_A['feature_type']\n",
        "\n",
        "    # Query the KDTree to find the nearest neighbor in gdf_type_B\n",
        "    distance, index = tree_B_original_crs.query(np.array([point_A_original_crs.x, point_A_original_crs.y]),\n",
        "                                                k=1, distance_upper_bound=DISTANCE_MAX_FOR_PLOT + 1)\n",
        "\n",
        "    if np.isfinite(distance):\n",
        "        # Get the corresponding point in WGS84 for plotting\n",
        "        point_A_wgs84 = gdf_type_A_wgs84.loc[idx_A].geometry\n",
        "        point_B_wgs84 = gdf_type_B_wgs84.iloc[index].geometry\n",
        "        uid_B = gdf_type_B_wgs84.iloc[index]['uid']\n",
        "        type_B = gdf_type_B_wgs84.iloc[index]['feature_type']\n",
        "\n",
        "        # Filter by the specified distance range for plotting\n",
        "        if DISTANCE_MIN_FOR_PLOT <= distance <= DISTANCE_MAX_FOR_PLOT:\n",
        "            plot_data_lines.append({\n",
        "                'source_uid': uid_A,\n",
        "                'source_type': type_A,\n",
        "                'source_lat': point_A_wgs84.y,\n",
        "                'source_lon': point_A_wgs84.x,\n",
        "                'target_uid': uid_B,\n",
        "                'target_type': type_B,\n",
        "                'target_lat': point_B_wgs84.y,\n",
        "                'target_lon': point_B_wgs84.x,\n",
        "                'distance_m': distance,\n",
        "            })\n",
        "\n",
        "if not plot_data_lines:\n",
        "    print(f\"No '{TARGET_FEATURE_TYPE_A}' instances found with a nearest '{TARGET_FEATURE_TYPE_B}' within {DISTANCE_MIN_FOR_PLOT}-{DISTANCE_MAX_FOR_PLOT}m.\")\n",
        "else:\n",
        "    df_plot_lines = pd.DataFrame(plot_data_lines)\n",
        "    print(f\"\\nFound {len(df_plot_lines)} pairs of '{TARGET_FEATURE_TYPE_A}' and '{TARGET_FEATURE_TYPE_B}' within the specified distance range.\")\n",
        "    print(\"\\nSample of calculated distances for plotting:\")\n",
        "    print(df_plot_lines[['source_uid', 'target_uid', 'distance_m']].head())\n",
        "\n",
        "\n",
        "    # --- 4. Plotting the Results using Folium (UPDATED SECTION) ---\n",
        "    print(\"\\nGenerating Folium map plot...\")\n",
        "\n",
        "    # Load the Sheffield boundary GeoJSON\n",
        "    try:\n",
        "        gdf_sheffield_boundary = gpd.read_file(SHEFFIELD_BOUNDARY_GEOJSON_PATH)\n",
        "        # Ensure the boundary is in WGS84 for Folium\n",
        "        gdf_sheffield_boundary_wgs84 = gdf_sheffield_boundary.to_crs(new_crs)\n",
        "        print(f\"Loaded Sheffield boundary from {SHEFFIELD_BOUNDARY_GEOJSON_PATH}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Warning: Sheffield boundary GeoJSON not found at {SHEFFIELD_BOUNDARY_GEOJSON_PATH}. Skipping boundary layer.\")\n",
        "        gdf_sheffield_boundary_wgs84 = None\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading Sheffield boundary: {e}. Skipping boundary layer.\")\n",
        "        gdf_sheffield_boundary_wgs84 = None\n",
        "\n",
        "\n",
        "    # Calculate map center (centroid of all features in WGS84, or use Sheffield boundary centroid)\n",
        "    if gdf_sheffield_boundary_wgs84 is not None and not gdf_sheffield_boundary_wgs84.empty:\n",
        "        map_center_lat = gdf_sheffield_boundary_wgs84.geometry.centroid.y.mean()\n",
        "        map_center_lon = gdf_sheffield_boundary_wgs84.geometry.centroid.x.mean()\n",
        "    else:\n",
        "        map_center_lat = all_features_gdf_wgs84.geometry.y.mean()\n",
        "        map_center_lon = all_features_gdf_wgs84.geometry.x.mean()\n",
        "\n",
        "    m = folium.Map(location=[map_center_lat, map_center_lon], zoom_start=12, control_scale=True)\n",
        "\n",
        "    # Create FeatureGroups for better organization in the map's layer control\n",
        "    all_features_group = folium.FeatureGroup(name='All Features (Context)').add_to(m)\n",
        "    type_A_group = folium.FeatureGroup(name=f'{TARGET_FEATURE_TYPE_A} Locations').add_to(m)\n",
        "    type_B_group = folium.FeatureGroup(name=f'{TARGET_FEATURE_TYPE_B} Locations').add_to(m)\n",
        "    lines_group = folium.FeatureGroup(name=f'Nearest Neighbors ({DISTANCE_MIN_FOR_PLOT}-{DISTANCE_MAX_FOR_PLOT}m)').add_to(m)\n",
        "\n",
        "    # --- Add Sheffield Boundary Layer ---\n",
        "    if gdf_sheffield_boundary_wgs84 is not None and not gdf_sheffield_boundary_wgs84.empty:\n",
        "        folium.GeoJson(\n",
        "            gdf_sheffield_boundary_wgs84.__geo_interface__,\n",
        "            name='Sheffield Boundary',\n",
        "            style_function=lambda x: {\n",
        "                'fillColor': '#cccccc', # Light grey fill\n",
        "                'color': 'black',      # Black border\n",
        "                'weight': 2,           # Border thickness\n",
        "                'fillOpacity': 0.1     # Semi-transparent fill\n",
        "            }\n",
        "        ).add_to(m) # Add directly to map or a dedicated feature group if desired\n",
        "\n",
        "    # Add all features as light grey circle markers for context\n",
        "    for idx, row in all_features_gdf_wgs84.iterrows():\n",
        "        folium.CircleMarker(\n",
        "            location=[row.geometry.y, row.geometry.x],\n",
        "            radius=2,\n",
        "            color='lightgrey',\n",
        "            fill=True,\n",
        "            fill_color='lightgrey',\n",
        "            fill_opacity=0.6,\n",
        "            tooltip=f\"{row['feature_type']} (UID: {row['uid']})\"\n",
        "        ).add_to(all_features_group)\n",
        "\n",
        "    # Add markers for TARGET_FEATURE_TYPE_A (e.g., Nursing Homes)\n",
        "    for idx, row in gdf_type_A_wgs84.iterrows():\n",
        "        folium.CircleMarker(\n",
        "            location=[row.geometry.y, row.geometry.x],\n",
        "            radius=5,\n",
        "            color='red',\n",
        "            fill=True,\n",
        "            fill_color='red',\n",
        "            fill_opacity=0.8,\n",
        "            tooltip=f\"{row['feature_type']} (UID: {row['uid']})\"\n",
        "        ).add_to(type_A_group)\n",
        "\n",
        "    # Add markers for TARGET_FEATURE_TYPE_B (e.g., Diffusion Tubes)\n",
        "    for idx, row in gdf_type_B_wgs84.iterrows():\n",
        "        folium.CircleMarker(\n",
        "            location=[row.geometry.y, row.geometry.x],\n",
        "            radius=5,\n",
        "            color='blue',\n",
        "            fill=True,\n",
        "            fill_color='blue',\n",
        "            fill_opacity=0.8,\n",
        "            tooltip=f\"{row['feature_type']} (UID: {row['uid']})\"\n",
        "        ).add_to(type_B_group)\n",
        "\n",
        "    # Add connecting lines\n",
        "    for idx, row in df_plot_lines.iterrows():\n",
        "        folium.PolyLine(\n",
        "            locations=[[row['source_lat'], row['source_lon']], [row['target_lat'], row['target_lon']]],\n",
        "            color='green',\n",
        "            weight=1.5,\n",
        "            opacity=0.7,\n",
        "            dash_array='5, 5', # Dashed line\n",
        "            tooltip=f\"Distance: {row['distance_m']:.2f}m<br>{row['source_type']} (UID:{row['source_uid']})<br>{row['target_type']} (UID:{row['target_uid']})\"\n",
        "        ).add_to(lines_group)\n",
        "\n",
        "    # Add layer control to the map\n",
        "    folium.LayerControl().add_to(m)\n",
        "\n",
        "    # Save the map as an HTML file\n",
        "    map_filename = f\"nearest_distances_folium_{TARGET_FEATURE_TYPE_A}_to_{TARGET_FEATURE_TYPE_B}_{DISTANCE_MIN_FOR_PLOT}-{DISTANCE_MAX_FOR_PLOT}m_with_boundary.html\"\n",
        "    map_filepath = os.path.join(output_dir_colocation, map_filename)\n",
        "    m.save(map_filepath)\n",
        "    print(f\"Interactive map saved to: {map_filepath}\")\n",
        "\n",
        "print(\"\\nDistance analysis and Folium plotting complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IsLnQFZA22IR"
      },
      "outputs": [],
      "source": [
        "# Display\n",
        "m"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "WqryKBE8UPjP",
        "e7D9Uf8ESqbX",
        "w58n-qckZvD5",
        "0nSPZubcF6Fp",
        "2uyiiNP6DOAK",
        "SnvsCdVoR_k4"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}